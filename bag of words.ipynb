{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALTEGRAD challenge Fall 2017\n",
    "Can you predict whether two short texts have the same meaning?\n",
    "\n",
    "https://www.kaggle.com/c/altegrad-challenge-fall-17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this competition is to predict which of the provided pairs of questions contain two questions with the same meaning.\n",
    "\n",
    "The ground truth is a set of labels supplied by human experts. This is inherently subjective, as the true meaning of sentences can not be known with certainty. Human labeling is a 'noisy' process, and different people would probably disagree. As a result, ground truth labels on this dataset should be taken as indications but not 100% accurate, and may include incorrect labeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/act444/lb-0-158-xgb-handcrafted-leaky\n",
    "https://github.com/Kulbear/quora-question-pair/tree/master/final_submission\n",
    "https://www.kaggle.com/c/quora-question-pairs/kernels\n",
    "https://www.kaggle.com/anokas/data-analysis-xgboost-starter-0-35460-lb\n",
    "https://www.kaggle.com/jturkewitz/magic-features-0-03-gain\n",
    "https://www.kaggle.com/cpmpml/spell-checker-using-word2vec\n",
    "https://www.kaggle.com/c/quora-question-pairs/discussion/34355\n",
    "https://www.kaggle.com/c/quora-question-pairs/discussion/34534\n",
    "https://github.com/rbauld/kaggle/blob/master/quora_question_pair/Features6.py\n",
    "http://chaire-dami.fr/files/2017/06/Quora_winning_solution.pdf\n",
    "https://medium.com/mlreview/implementing-malstm-on-kaggles-quora-question-pairs-competition-8b31b0b16a07\n",
    "https://towardsdatascience.com/identifying-duplicate-questions-on-quora-top-12-on-kaggle-4c1cf93f1c30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jolyanne\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\Jolyanne\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# General\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import re \n",
    "import itertools\n",
    "import operator\n",
    "import copy\n",
    "import heapq\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import cosine, euclidean, jaccard\n",
    "import os\n",
    "import warnings\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaMulticore\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.metrics.pairwise import cosine_distances, euclidean_distances\n",
    "from fuzzywuzzy import fuzz\n",
    "from jellyfish import jaro_distance, jaro_winkler\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import spacy\n",
    "from collections import Counter\n",
    "#spacy.load('en')\n",
    "#nlp = spacy.load('en')\n",
    "#nlp = spacy.load('en', parser=False)\n",
    "nlp = spacy.load('en', disable=['parser'])\n",
    "#In the default models, the parser is loaded and enabled as part of the standard processing pipeline. \n",
    "#If you don't need any of the syntactic information, you should disable the parser. \n",
    "#Disabling the parser will make spaCy load and run much faster.\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all_texts = pd.read_csv('train.csv', names=['row_ID', 'text_a_ID', 'text_b_ID', 'text_a_text', 'text_b_text', 'target'])\n",
    "### rajuter traitement stem et stopword sur df_all_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_ID</th>\n",
       "      <th>text_a_ID</th>\n",
       "      <th>text_b_ID</th>\n",
       "      <th>text_a_text</th>\n",
       "      <th>text_b_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>199954</td>\n",
       "      <td>384085</td>\n",
       "      <td>What are the some of the best novels?</td>\n",
       "      <td>What are some of the greatest novels of all ti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>128681</td>\n",
       "      <td>237407</td>\n",
       "      <td>What are the pictures that made you look twice?</td>\n",
       "      <td>What are some amazing pictures one has to see ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>170846</td>\n",
       "      <td>240621</td>\n",
       "      <td>Have the ellectoral college members ever voted...</td>\n",
       "      <td>When has the electoral college voted against t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>55110</td>\n",
       "      <td>177468</td>\n",
       "      <td>Did Ravana really have 10 heads?</td>\n",
       "      <td>Why did Ravana have 10 heads?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>425513</td>\n",
       "      <td>400256</td>\n",
       "      <td>What's a book that you feel helped you to impr...</td>\n",
       "      <td>What books or magazines should I read to impro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_ID  text_a_ID  text_b_ID  \\\n",
       "0       0     199954     384085   \n",
       "1       1     128681     237407   \n",
       "2       2     170846     240621   \n",
       "3       3      55110     177468   \n",
       "4       4     425513     400256   \n",
       "\n",
       "                                         text_a_text  \\\n",
       "0              What are the some of the best novels?   \n",
       "1    What are the pictures that made you look twice?   \n",
       "2  Have the ellectoral college members ever voted...   \n",
       "3                   Did Ravana really have 10 heads?   \n",
       "4  What's a book that you feel helped you to impr...   \n",
       "\n",
       "                                         text_b_text  target  \n",
       "0  What are some of the greatest novels of all ti...       0  \n",
       "1  What are some amazing pictures one has to see ...       0  \n",
       "2  When has the electoral college voted against t...       1  \n",
       "3                      Why did Ravana have 10 heads?       1  \n",
       "4  What books or magazines should I read to impro...       0  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv', names=['row_ID', 'text_a_ID', 'text_b_ID', 'text_a_text', 'text_b_text', 'target'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80100, 6)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(train, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56070, 6)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24030, 6)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20179, 6)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# real test data with no labels\n",
    "real_test = pd.read_csv('test.csv', names=['row_ID', 'text_a_ID', 'text_b_ID', 'text_a_text', 'text_b_text', 'target'])\n",
    "real_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text_simple(text, remove_stopwords=True, pos_filtering=True, stemming=True):\n",
    "    #print(text)\n",
    "    english_stopwords = set([stopword for stopword in stopwords.words('english')])\n",
    "    punct = set(string.punctuation)\n",
    "    punct.update([\"``\", \"`\", \"...\"])\n",
    "    text = text.lower()\n",
    "    text = ''.join(l for l in text if l not in punct) # remove punctuation (preserving intra-word dashes)\n",
    "    text = re.sub(' +',' ',text) # strip extra white space\n",
    "    text = text.strip() # strip leading and trailing white space\n",
    "    # tokenize (split based on whitespace)\n",
    "    tokens = text.split(' ')\n",
    "    if pos_filtering == True:\n",
    "        # POS tag and retain only nouns and adjectives\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "        tokens_keep = []\n",
    "        for item in tagged_tokens:\n",
    "            if (\n",
    "            item[1] == 'NN' or\n",
    "            item[1] == 'NNS' or\n",
    "            item[1] == 'NNP' or\n",
    "            item[1] == 'NNPS' or\n",
    "            item[1] == 'JJ' or\n",
    "            item[1] == 'JJS' or\n",
    "            item[1] == 'JJR'\n",
    "            ):\n",
    "                tokens_keep.append(item[0])\n",
    "        tokens = tokens_keep\n",
    "    if remove_stopwords:\n",
    "        # remove stopwords\n",
    "        tokens = [token for token in tokens if token not in english_stopwords and len(token)>1]\n",
    "    if stemming:\n",
    "        # apply Porter's stemmer\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        tokens_stemmed = list()\n",
    "        for token in tokens:\n",
    "            tokens_stemmed.append(stemmer.stem(token))\n",
    "        tokens = tokens_stemmed\n",
    "    \n",
    "    return(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of pairs ans texts array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_pairs(train, test, remove_stopwords= True, pos_filtering=False, stemming = True):\n",
    "    texts = {}\n",
    "    pairs_train = []\n",
    "    pairs_test = []\n",
    "    y_train = []\n",
    "    y_true = []\n",
    "    ids2ind = {} # will contain the row idx of each unique text in the TFIDF matrix \n",
    "\n",
    "    for idx, l in enumerate(train.values):\n",
    "        if l[1] not in texts:\n",
    "            texts[l[1]] = clean_text_simple(l[3], remove_stopwords = remove_stopwords, \n",
    "                                            pos_filtering = pos_filtering, \n",
    "                                            stemming = stemming)\n",
    "            \n",
    "        if l[2] not in texts:\n",
    "            texts[l[2]] = clean_text_simple(l[4], remove_stopwords = remove_stopwords, \n",
    "                                            pos_filtering = pos_filtering, \n",
    "                                            stemming = stemming)\n",
    "\n",
    "        pairs_train.append([l[1], l[2]])\n",
    "        y_train.append(int(l[5]))\n",
    "\n",
    "    for idx, l in enumerate(test.values):\n",
    "        if l[1] not in texts:\n",
    "            texts[l[1]] = clean_text_simple(l[3], remove_stopwords = remove_stopwords, \n",
    "                                            pos_filtering = pos_filtering, \n",
    "                                            stemming = stemming)\n",
    "            \n",
    "        if l[2] not in texts:\n",
    "            texts[l[2]] = clean_text_simple(l[4], remove_stopwords = remove_stopwords, \n",
    "                                            pos_filtering = pos_filtering, \n",
    "                                            stemming = stemming)\n",
    "\n",
    "        pairs_test.append([l[1], l[2]])\n",
    "        y_true.append(int(l[5])) \n",
    "        \n",
    "    for qid in texts:\n",
    "        ids2ind[qid] = len(ids2ind)\n",
    "    \n",
    "    return texts,pairs_train, pairs_test, y_train, y_true, ids2ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tfIdf(texts):\n",
    "    return TfidfVectorizer().fit_transform(texts.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy tags\n",
    "https://spacy.io/usage/linguistic-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spacy_tag(sentence):\n",
    "    sentence = nlp(sentence)\n",
    "    count_tags = Counter([w.pos_ for w in sentence])\n",
    "    return count_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity(count_tags1, count_tags2):\n",
    "    bag_of_tags1 = list(count_tags1.keys())\n",
    "    #print(bag_of_tags1)\n",
    "    bag_of_tags_values1 = [count_tags1.get(l) for l in bag_of_tags1]\n",
    "    #print(bag_of_tags_values1)\n",
    "    bag_of_tags2 = list(count_tags2.keys())\n",
    "    #print(bag_of_tags2)\n",
    "    bag_of_tags_values2 = [count_tags2.get(l) for l in bag_of_tags2]\n",
    "    #print(bag_of_tags_values2)\n",
    "    \n",
    "    everseen = list()\n",
    "    diff = 0\n",
    "    for i, tag in enumerate(bag_of_tags1):\n",
    "        if tag in bag_of_tags2:\n",
    "            everseen.append(tag)\n",
    "            index = bag_of_tags2.index(tag)\n",
    "            diff = diff + np.abs(bag_of_tags_values1[i] - bag_of_tags_values2[index])\n",
    "        else :\n",
    "            everseen.append(tag)\n",
    "            diff = diff + bag_of_tags_values1[i]\n",
    "    \n",
    "    for i, tag in enumerate(bag_of_tags2):\n",
    "        if tag not in everseen:\n",
    "            everseen.append(tag)\n",
    "            diff = diff + bag_of_tags_values2[i]\n",
    "    #print(diff)        \n",
    "    #return diff / len(everseen)\n",
    "    return diff / (np.sum(bag_of_tags_values1) + np.sum(bag_of_tags_values2)) # We normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numbers of Words in common and difference between 2 questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def common_words(q1, q2):\n",
    "    q1 = q1.split(' ')\n",
    "    q2 = q2.split(' ')\n",
    "    everseen = list()\n",
    "    common = 0\n",
    "    for i, tag in enumerate(q1):\n",
    "        if tag in q2:\n",
    "            everseen.append(tag)\n",
    "            common = common + 1\n",
    "        else :\n",
    "            everseen.append(tag)\n",
    "    return common\n",
    "    \n",
    "def diff_words(q1, q2):\n",
    "    q1 = q1.split(' ')\n",
    "    q2 = q2.split(' ')\n",
    "    everseen = list()\n",
    "    diff = 0\n",
    "    for tag in q1:\n",
    "        if tag not in q2:\n",
    "            everseen.append(tag)\n",
    "            diff = diff + 1\n",
    "        else :\n",
    "            everseen.append(tag)          \n",
    "    for tag in q2:\n",
    "        if tag not in (everseen and q2):\n",
    "            everseen.append(tag)\n",
    "            diff = diff + 1\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy String Matching\n",
    "Calculate edit distances between each question pair (Levenshtein, Jaro, Jaro-Winkler, ...).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fuzzy(q1_text, q2_text):   \n",
    "    q1_tokens=q1_text.split()\n",
    "    q2_tokens=q2_text.split()\n",
    "    fuzzy_distances = np.array([\n",
    "        fuzz.ratio(q1_tokens, q2_tokens),\n",
    "        fuzz.partial_ratio(q1_tokens, q2_tokens),\n",
    "        fuzz.token_sort_ratio(q1_tokens, q2_tokens),\n",
    "        fuzz.token_set_ratio(q1_tokens, q2_tokens),\n",
    "        fuzz.partial_token_sort_ratio(q1_tokens, q2_tokens),\n",
    "    ], dtype='float')\n",
    "    \n",
    "    # Normalize to [0 - 1] range.\n",
    "    fuzzy_distances /= 100\n",
    "    \n",
    "    jelly_distances = np.array([\n",
    "        jaro_distance(q1_text, q2_text),\n",
    "        jaro_winkler(q1_text, q2_text),\n",
    "    ])\n",
    "    \n",
    "    return np.concatenate([fuzzy_distances, jelly_distances])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character N-Gram Jaccard Index\n",
    "Calculate Jaccard similarities between sets of character $n$-grams for different values of $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NGRAM_RANGE = range(2, 6)\n",
    "\n",
    "def get_char_ngrams(doc, n):\n",
    "    return [doc[i:i + n] for i in range(len(doc) - n + 1)]\n",
    "\n",
    "def get_jaccard_set_similarities(a, b):\n",
    "    len_intersection = len(a.intersection(b))\n",
    "    jaccard_index = len_intersection / len(a.union(b))\n",
    "    jaccard_index_norm_a = len_intersection / len(a)\n",
    "    jaccard_index_norm_b = len_intersection / len(b)\n",
    "    \n",
    "    return jaccard_index, jaccard_index_norm_a, jaccard_index_norm_b\n",
    "\n",
    "def get_jaccard_similarities(q1, q2, n):\n",
    "    if len(q1) < max(NGRAM_RANGE) and len(q2) < max(NGRAM_RANGE):\n",
    "        return 1, 1, 1\n",
    "    if len(q1) < max(NGRAM_RANGE) or len(q2) < max(NGRAM_RANGE):\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    q1_ngrams = set(get_char_ngrams(q1, n))\n",
    "    q2_ngrams = set(get_char_ngrams(q2, n))\n",
    "    return get_jaccard_set_similarities(q1_ngrams, q2_ngrams)\n",
    "\n",
    "def get_question_pair_features(q1,q2):\n",
    "    \n",
    "    features = []\n",
    "    for n in NGRAM_RANGE:\n",
    "        features.extend(get_jaccard_similarities(q1, q2, n))\n",
    "    \n",
    "    return features\n",
    "#a=get_question_pair_features(\"bonjour petit chat noir\",\"salut petit chien blanc\" )\n",
    "#a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### LDA Topic Distances\n",
    "Train a Latent Dirichlet Allocation model with 300 topics on the question corpus and compute topic distances between the question pairs.\n",
    "https://github.com/YuriyGuts/kaggle-quora-question-pairs/blob/master/notebooks/feature-lda.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem_pair(pair):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    return [\n",
    "        [stemmer.stem(token) for token in texts[pair[0]].split()],\n",
    "        [stemmer.stem(token) for token in texts[pair[1]].split()],\n",
    "    ]\n",
    "\n",
    "def compute_topic_distances(q1, q2, lda_dictionary, model):\n",
    "    \n",
    "    q1_bow = lda_dictionary.doc2bow(q1)\n",
    "    q2_bow = lda_dictionary.doc2bow(q2)\n",
    "    \n",
    "    q1_topic_vec = np.array(model.get_document_topics(q1_bow, minimum_probability=0))[:, 1].reshape(1, -1)\n",
    "    q2_topic_vec = np.array(model.get_document_topics(q2_bow, minimum_probability=0))[:, 1].reshape(1, -1)\n",
    "    \n",
    "    return [\n",
    "        cosine_distances(q1_topic_vec, q2_topic_vec)[0][0],\n",
    "        euclidean_distances(q1_topic_vec, q2_topic_vec)[0][0],\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda_init(pairs_train, pairs_test, df_all_texts):\n",
    "    NUM_TOPICS = 300\n",
    "    RANDOM_SEED = 42\n",
    "    #stemmer = SnowballStemmer('english')\n",
    "\n",
    "    lda_tokens = list()\n",
    "\n",
    "    for i in range(len(pairs_train)):\n",
    "        lda_tokens.append(stem_pair(pairs_train[i]))\n",
    "    for i in range(len(pairs_test)):\n",
    "        lda_tokens.append(stem_pair(pairs_test[i]))\n",
    "\n",
    "    lda_documents = list(np.array(lda_tokens).ravel()) #When a view is desired, arr.reshape(-1) may be preferable.\n",
    "    lda_dictionary = Dictionary(lda_documents)\n",
    "    lda_corpus = [lda_dictionary.doc2bow(document) for document in lda_documents]\n",
    "\n",
    "    model = LdaMulticore(\n",
    "        lda_corpus,\n",
    "        num_topics=NUM_TOPICS,\n",
    "        id2word=lda_dictionary,\n",
    "        random_state=RANDOM_SEED,\n",
    "    )\n",
    "\n",
    "    lda_distances = list()\n",
    "    for i in lda_tokens:\n",
    "        lda_distances.append(compute_topic_distances(i[0], i[1],lda_dictionary, model))\n",
    "\n",
    "    lda_train = np.array(lda_distances[:len(pairs_train)], dtype='float64')\n",
    "    lda_test = np.array(lda_distances[len(pairs_train):], dtype='float64')\n",
    "    \n",
    "    columns_lda=['lda_1','lda_2']\n",
    "    lda_distances = pd.DataFrame(\n",
    "    lda_distances,\n",
    "    columns=columns_lda\n",
    "    )\n",
    "\n",
    "    lda_df = pd.concat([lda_distances, df_all_texts['text_a_ID'], df_all_texts['text_b_ID'] ], axis=1)\n",
    "    return lda_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lda(q1,q2, lda_df):\n",
    "    \n",
    "    raw = lda_df[lda_df['text_a_ID'] == q1][lda_df[lda_df['text_a_ID'] == q1]['text_b_ID'] == q2]\n",
    "    raw1 = lda_df[lda_df['text_a_ID'] == q2][lda_df[lda_df['text_a_ID'] == q2]['text_b_ID'] == q1]\n",
    "\n",
    "    if(raw1.empty): return raw\n",
    "    elif(raw.empty): return raw1\n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS/NER Tag Similarity\n",
    "Derive bag-of-POS-tag (part of speech tagging) and bag-of-NER-tag (Named Entity Recognition) vectors from each question and calculate their vector distances.\n",
    "\n",
    "(POS) https://spacy.io/usage/linguistic-features#pos-tagging\n",
    "\n",
    "(NER) https://spacy.io/usage/linguistic-features#named-entities\n",
    "\n",
    "\n",
    "to see the meaning of tags: https://spacy.io/api/annotation#pos-tagging\n",
    "\n",
    "(NER) labels sequences of words in a text which are the names of things, such as person and company names, or gene and protein names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_counter(name, df_all_texts, num_raw_features, pos_tags_whitelist, ner_tags_whitelist):\n",
    "    \n",
    "    X1 = np.zeros((len(df_all_texts), num_raw_features))\n",
    "    pipe_q1 = nlp.pipe(df_all_texts[name].values, n_threads=os.cpu_count())\n",
    "\n",
    "    for i, doc in enumerate(pipe_q1):\n",
    "        pos_counter = Counter(token.pos_ for token in doc)\n",
    "        ner_counter = Counter(ent.label_ for ent in doc.ents)\n",
    "        X1[i, :] = np.array(\n",
    "            [pos_counter[pos_tag] for pos_tag in pos_tags_whitelist] +\n",
    "            [ner_counter[ner_tag] for ner_tag in ner_tags_whitelist]\n",
    "        )\n",
    "    return X1\n",
    "\n",
    "def get_vector_distances(i, X1, X2, pos_tags_whitelist, ner_tags_whitelist):\n",
    "    return [\n",
    "        # POS distances.\n",
    "        cosine(X1[i, 0:len(pos_tags_whitelist)], X2[i, 0:len(pos_tags_whitelist)]),\n",
    "        euclidean(X1[i, 0:len(pos_tags_whitelist)], X2[i, 0:len(pos_tags_whitelist)]),\n",
    "\n",
    "        # NER distances.\n",
    "        euclidean(X1[i, -len(ner_tags_whitelist):], X2[i, -len(ner_tags_whitelist):]),\n",
    "        np.abs(np.sum(X1[i, -len(ner_tags_whitelist):]) - np.sum(X2[i, -len(ner_tags_whitelist):])),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_ner_tags(df_all_texts):\n",
    "    pos_tags_whitelist = ['ADJ', 'ADV', 'NOUN', 'PROPN', 'NUM', 'VERB']\n",
    "    ner_tags_whitelist = ['GPE', 'LOC', 'ORG', 'NORP', 'PERSON', 'PRODUCT', 'DATE', 'TIME', 'QUANTITY', 'CARDINAL']\n",
    "\n",
    "    num_raw_features = len(pos_tags_whitelist) + len(ner_tags_whitelist)\n",
    "\n",
    "    X1 = create_counter('text_a_text', df_all_texts, num_raw_features, pos_tags_whitelist, ner_tags_whitelist)\n",
    "    X2 = create_counter('text_b_text', df_all_texts, num_raw_features, pos_tags_whitelist, ner_tags_whitelist)\n",
    "\n",
    "    df_pos_q1 = pd.DataFrame(\n",
    "        X1[:, 0:len(pos_tags_whitelist)],\n",
    "        columns=['pos_q1_' + pos_tag.lower() for pos_tag in pos_tags_whitelist])\n",
    "    df_pos_q2 = pd.DataFrame(\n",
    "        X2[:, 0:len(pos_tags_whitelist)],\n",
    "        columns=['pos_q2_' + pos_tag.lower() for pos_tag in pos_tags_whitelist])\n",
    "    df_ner_q1 = pd.DataFrame(\n",
    "        X1[:, -len(ner_tags_whitelist):],\n",
    "        columns=['ner_q1_' + ner_tag.lower() for ner_tag in ner_tags_whitelist])\n",
    "    df_ner_q2 = pd.DataFrame(\n",
    "        X2[:, -len(ner_tags_whitelist):],\n",
    "        columns=['ner_q2_' + ner_tag.lower() for ner_tag in ner_tags_whitelist])\n",
    "    \n",
    "    \n",
    "    tags_distances = list()\n",
    "    for i in list(range(len(df_all_texts))):\n",
    "        tags_distances.append(get_vector_distances(i,X1, X2, pos_tags_whitelist, ner_tags_whitelist))\n",
    "\n",
    "    tags_columns=[\n",
    "            'pos_tag_cosine',\n",
    "            'pos_tag_euclidean',\n",
    "            'ner_tag_euclidean',\n",
    "            'ner_tag_count_diff',\n",
    "        ]\n",
    "\n",
    "    tags_distances = pd.DataFrame(tags_distances, columns = tags_columns)\n",
    "    df_tags = pd.concat([tags_distances, df_all_texts['text_a_ID'], df_all_texts['text_b_ID'] ], axis=1)\n",
    "    return df_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tags(q1, q2, df_tags):\n",
    "    raw = df_tags[df_tags['text_a_ID'] == q1][df_tags[df_tags['text_a_ID'] == q1]['text_b_ID'] == q2]\n",
    "    raw1 = df_tags[df_tags['text_a_ID'] == q2][df_tags[df_tags['text_a_ID'] == q2]['text_b_ID'] == q1]\n",
    "\n",
    "    if(raw1.empty): return raw\n",
    "    elif(raw.empty): return raw1\n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frequency of questions\n",
    "\n",
    "more frequent questions are more likely to have the same meaning ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_question_freq(train, test):\n",
    "\n",
    "    df1 = train[['text_a_text']].copy()\n",
    "    df2 = train[['text_b_text']].copy()\n",
    "    df1_test = test[['text_a_text']].copy()\n",
    "    df2_test = test[['text_b_text']].copy()\n",
    "\n",
    "    df2.rename(columns = {'question2':'question1'},inplace=True)\n",
    "    df2_test.rename(columns = {'question2':'question1'},inplace=True)\n",
    "\n",
    "    train_questions = df1.append(df2)\n",
    "    train_questions = train_questions.append(df1_test)\n",
    "    train_questions = train_questions.append(df2_test)\n",
    "    train_questions.drop_duplicates(subset = ['text_a_text'],inplace=True)\n",
    "    train_questions.reset_index(inplace=True,drop=True)\n",
    "    questions_dict = pd.Series(train_questions.index.values,index=train_questions.text_a_text.values).to_dict()\n",
    "\n",
    "    train_cp = train.copy()\n",
    "    test_cp = test.copy()\n",
    "    train_cp.drop(['text_a_ID','text_b_ID'],axis=1,inplace=True)\n",
    "    test_cp.drop(['text_a_ID','text_b_ID'],axis=1,inplace=True)\n",
    "    test_cp['target'] = -1\n",
    "    #test_cp.rename(columns={'row_ID':'id'},inplace=True)\n",
    "\n",
    "    comb = pd.concat([train_cp,test_cp])\n",
    "    comb['q1_hash'] = comb['text_a_text'].map(questions_dict)\n",
    "    comb['q2_hash'] = comb['text_b_text'].map(questions_dict)\n",
    "\n",
    "    q1_vc = comb.q1_hash.value_counts().to_dict()\n",
    "    q2_vc = comb.q2_hash.value_counts().to_dict()\n",
    "\n",
    "    def try_apply_dict(x,dict_to_apply):\n",
    "        try:\n",
    "            return dict_to_apply[x]\n",
    "        except KeyError:\n",
    "            return 0\n",
    "\n",
    "    comb['q1_freq'] = comb['q1_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "    comb['q2_freq'] = comb['q2_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "\n",
    "    train_comb = comb[comb['target'] >= 0][['row_ID','q1_hash','q2_hash','q1_freq','q2_freq','target']]\n",
    "    test_comb = comb[comb['target'] < 0][['row_ID','q1_hash','q2_hash','q1_freq','q2_freq']]\n",
    "\n",
    "    #corr_mat = train_comb.corr()\n",
    "    #corr_mat.head()\n",
    "    \n",
    "    train_comb.reset_index(inplace=True,drop=True)\n",
    "    test_comb.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    return train_comb.sort_index(), test_comb.sort_index()\n",
    "    #return train_comb, test_comb\n",
    "\n",
    "#train_freq, test_freq = compute_question_freq(train, test)\n",
    "#train_freq.head()\n",
    "#train.loc[train['row_ID']== 26790]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_match_share(q1, q2):\n",
    "    stops = set([stopword for stopword in stopwords.words('english')])\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in q1:\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in q2:\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "def jaccard(q1, q2):\n",
    "    wic = set(q1).intersection(set(q2))\n",
    "    uw = set(q1).union(q2)\n",
    "    if len(uw) == 0:\n",
    "        uw = [1]\n",
    "    return (len(wic) / len(uw))\n",
    "\n",
    "def common_words(q1, q2):\n",
    "    return len(set(q1).intersection(set(q2)))\n",
    "\n",
    "def total_unique_words(q1, q2):\n",
    "    return len(set(q1).union(q2))\n",
    "\n",
    "def total_unq_words_stop(q1, q2):\n",
    "    stops = set([stopword for stopword in stopwords.words('english')])\n",
    "    return len([x for x in set(q1).union(q2) if x not in stops])\n",
    "\n",
    "def wc_diff(q1, q2):\n",
    "    return abs(len(q1) - len(q2))\n",
    "\n",
    "def wc_ratio(q1, q2):\n",
    "    l1 = len(q1)*1.0 \n",
    "    l2 = len(q2)\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "    \n",
    "def wc_diff_unique(q1, q2):\n",
    "    return abs(len(set(q1)) - len(set(q2)))\n",
    "\n",
    "def wc_ratio_unique(q1, q2):\n",
    "    l1 = len(set(q1)) * 1.0\n",
    "    l2 = len(set(q2))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def wc_diff_unique_stop(q1, q2):\n",
    "    stops = set([stopword for stopword in stopwords.words('english')])\n",
    "    return abs(len([x for x in set(q1) if x not in stops]) - len([x for x in set(q2) if x not in stops]))\n",
    "\n",
    "def wc_ratio_unique_stop(q1, q2):\n",
    "    stops = set([stopword for stopword in stopwords.words('english')])\n",
    "    l1 = len([x for x in set(q1) if x not in stops])*1.0 \n",
    "    l2 = len([x for x in set(q2) if x not in stops])\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def same_start_word(q1, q2):\n",
    "    if not q1 or not q2:\n",
    "        return np.nan\n",
    "    return int(q1[0] == q2[0])\n",
    "\n",
    "def char_diff(q1, q2):\n",
    "    return abs(len(''.join(q1)) - len(''.join(q2)))\n",
    "\n",
    "def char_ratio(q1, q2):\n",
    "    l1 = len(''.join(q1)) \n",
    "    l2 = len(''.join(q2))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def char_diff_unique_stop(q1, q2):\n",
    "    stops = set([stopword for stopword in stopwords.words('english')])\n",
    "    return abs(len(''.join([x for x in set(q1) if x not in stops])) - len(''.join([x for x in set(q2) if x not in stops])))\n",
    "\n",
    "\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "\n",
    "\n",
    "def tfidf_word_match_share_stops(row, weights=None):\n",
    "    stops = set([stopword for stopword in stopwords.words('english')])\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in row['question1']:\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in row['question2']:\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "def tfidf_word_match_share(q1, q2, weights=None):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in q1:\n",
    "        q1words[word] = 1\n",
    "    for word in q2:\n",
    "        q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We save into csv files some features in case of problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_lda_df(lda_df, name):\n",
    "    with open(name, 'w') as f:\n",
    "        f.write(\"lda_1,lda_2,text_a_ID,text_b_ID\\n\")\n",
    "        for i in range(len(lda_df)):\n",
    "            f.write(str(lda_df['lda_1'][i])\n",
    "                    +','\n",
    "                    +str(lda_df['lda_2'][i])\n",
    "                    +','\n",
    "                    +str(lda_df['text_a_ID'][i])\n",
    "                    +','\n",
    "                    +str(lda_df['text_b_ID'][i])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_df_tags(df_tags, name):\n",
    "    with open(name, 'w') as f:\n",
    "        f.write(\"pos_tag_cosine,pos_tag_euclidean,ner_tag_euclidean,ner_tag_count_diff,text_a_ID,text_b_ID\\n\")\n",
    "        for i in range(len(df_tags)):\n",
    "            f.write(str(df_tags['pos_tag_cosine'][i])\n",
    "                    +','\n",
    "                    +str(df_tags['pos_tag_euclidean'][i])\n",
    "                    +','\n",
    "                    +str(df_tags['ner_tag_euclidean'][i])\n",
    "                    +','\n",
    "                    +str(df_tags['ner_tag_count_diff'][i])\n",
    "                    +','\n",
    "                    +str(df_tags['text_a_ID'][i])\n",
    "                    +','\n",
    "                    +str(df_tags['text_b_ID'][i])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features name\n",
    "features=['cosine similarity (CS)',\n",
    "          'total length (TL)', \n",
    "          'difference length (DL)',\n",
    "          'sim pos tags (POS)', \n",
    "          'POS*DL',\n",
    "          'POS*TL', \n",
    "          'POS*POS*TL',\n",
    "          'POS*POS*DL',\n",
    "          'common words (CW)', \n",
    "          'CW*POS','CW*POS*DL',\n",
    "          'CW*POS*TL',\n",
    "          'fuzz ratio',\n",
    "          'fuzz partial_ratio',\n",
    "          'fuzz token_sort_ratio',\n",
    "          'fuzz token_set_ratio',\n",
    "          'fuzz partial_token_sort_ratio',\n",
    "          'jaro_distance',\n",
    "          'jaro_winkler',\n",
    "          'jaccard_index n=2',\n",
    "          'jaccard_index_norm_a n=2',\n",
    "          'jaccard_index_norm_b n=2',\n",
    "          'jaccard_index n=3',\n",
    "          'jaccard_index_norm_a n=3', \n",
    "          'jaccard_index_norm_b n=3',\n",
    "          'jaccard_index n=4', \n",
    "          'jaccard_index_norm_a n=4', \n",
    "          'jaccard_index_norm_b n=4',\n",
    "          'jaccard_index n=5', \n",
    "          'jaccard_index_norm_a n=5', \n",
    "          'jaccard_index_norm_b n=5',\n",
    "          'pos_tag_cosine',\n",
    "          'pos_tag_euclidean',\n",
    "          'ner_tag_euclidean',\n",
    "          'ner_tag_count_diff',\n",
    "          'diff_words',\n",
    "          'lda_1',\n",
    "          'lda_2',\n",
    "          'common_diff',\n",
    "          'common_min',\n",
    "          'common_max',\n",
    "          'mean_l',\n",
    "          'same_last',\n",
    "          'same_first',\n",
    "          'freq_q1',\n",
    "          'freq_q2',\n",
    "         '1','2','3','4','5','1','2','3','4','5','1','2','3','4','5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# faire varier la construction des pairs avec steeming, ...\n",
    "def construct_data(pairs_train, A, lda_df, df_tags, train_freq):\n",
    "    \n",
    "    N_train = len(pairs_train)\n",
    "    X_train = np.zeros((N_train, 61))\n",
    "    SAFE_DIV = 0.0001\n",
    "\n",
    "    for i in range(N_train):\n",
    "        if i%1000 == 0: print(i) #pour suivre la progression\n",
    "            \n",
    "        q1 = pairs_train[i][0]\n",
    "        q2 = pairs_train[i][1]\n",
    "        \n",
    "        X_train[i, 0] = cosine_similarity(A[ids2ind[q1], :], A[ids2ind[q2], :])  \n",
    "        X_train[i, 1] = len(texts[q1]) + len(texts[q2])\n",
    "        X_train[i, 2] = abs(len(texts[q1]) - len(texts[q2])) \n",
    "        \n",
    "        X_train[i, 3] = similarity(spacy_tag(texts[q1]), spacy_tag(texts[q2])) # similarity on spacy tag: noun, verb, ...\n",
    "        X_train[i, 4] = X_train[i, 2] * X_train[i, 3]\n",
    "        X_train[i, 5] = X_train[i, 1] * X_train[i, 3]\n",
    "        X_train[i, 6] = X_train[i, 5] * X_train[i, 3]\n",
    "        X_train[i, 7] = X_train[i, 4] * X_train[i, 3]\n",
    "        \n",
    "        X_train[i, 8] = common_words(texts[q1], texts[q2]) # number of words in common\n",
    "        X_train[i, 9] = X_train[i, 8] * X_train[i, 3]\n",
    "        X_train[i, 10] = X_train[i, 8] * X_train[i, 4]\n",
    "        X_train[i, 11] = X_train[i, 8] * X_train[i, 5]\n",
    "        \n",
    "        a = fuzzy(texts[q1], texts[q2])\n",
    "        for j in range(7):\n",
    "            X_train[i, 12 + j] = a[j]\n",
    "            \n",
    "        b = get_question_pair_features(texts[q1], texts[q2])\n",
    "        for j in range(12):\n",
    "            X_train[i, 19 + j] = b[j]\n",
    "            \n",
    "        c = get_tags(q1, q2, df_tags)\n",
    "        tags_columns=[\n",
    "            'pos_tag_cosine',\n",
    "            'pos_tag_euclidean',\n",
    "            'ner_tag_euclidean',\n",
    "            'ner_tag_count_diff',\n",
    "        ]\n",
    "        for j in range(4):\n",
    "            X_train[i, 31 + j] = c[tags_columns[j]]\n",
    "            \n",
    "        X_train[i, 35] = diff_words(texts[q1], texts[q2]) # different number of words aussi égal à: X_train[i, 1] - X_train[i, 8]\n",
    "    \n",
    "        columns_lda=['lda_1','lda_2']\n",
    "        d = get_lda(q1, q2, lda_df)\n",
    "        for j in range(2):\n",
    "            X_train[i, 36 + j] = d[columns_lda[j]]\n",
    "            \n",
    "        X_train[i, 38] = X_train[i, 8] / (X_train[i, 35] + SAFE_DIV) # ratio nb words in common, nb words differents\n",
    "        X_train[i, 39] = X_train[i, 8] / (min(len(texts[q1].split()), len(texts[q2].split())) + SAFE_DIV) # common on min\n",
    "        X_train[i, 40] = X_train[i, 8] / (max(len(texts[q1].split()), len(texts[q2].split())) + SAFE_DIV) # common on max\n",
    "        X_train[i, 41] = (len(texts[q1].split()) + len(texts[q2].split()))/2 # mean lenght of words\n",
    "        \n",
    "        if len(texts[q1].split()) != 0 and len(texts[q2].split()) != 0:\n",
    "            X_train[i, 42] = int(texts[q1].split()[-1] == texts[q2].split()[-1]) # same last word\n",
    "            X_train[i, 43] = int(texts[q1].split()[0] == texts[q2].split()[0]) # same first word\n",
    "            \n",
    "        X_train[i, 44] = train_freq.loc[i]['q1_freq']\n",
    "        X_train[i, 45] = train_freq.loc[i]['q2_freq']\n",
    "        \n",
    "        X_train[i, 46] = word_match_share(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 47] = jaccard(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 48] = common_words(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 49] = total_unique_words(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 50] = total_unq_words_stop(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 51] = wc_diff(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 52] = wc_ratio(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 53] = wc_diff_unique(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 54] = wc_ratio_unique(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 55] = wc_diff_unique_stop(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 56] = wc_ratio_unique_stop(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 57] = same_start_word(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 58] = char_diff(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 59] = char_ratio(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 60] = char_diff_unique_stop(texts[q1].split(), texts[q2].split())\n",
    "\n",
    "          \n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs construction\n"
     ]
    }
   ],
   "source": [
    "print('Pairs construction')\n",
    "texts, pairs_train, pairs_test, y_train, y_true, ids2ind = construct_pairs(train, \n",
    "                                                                          test, \n",
    "                                                                          remove_stopwords = True, \n",
    "                                                                          pos_filtering = False, \n",
    "                                                                          stemming = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    }
   ],
   "source": [
    "print('A')\n",
    "A = tfIdf(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('lda')\n",
    "lda_df = lda_init(pairs_train, pairs_test, df_all_texts)\n",
    "save_lda_df(lda_df, \"lda_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('tags')\n",
    "df_tags = pos_ner_tags(df_all_texts)\n",
    "save_df_tags(df_tags,\"df_tags.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq\n"
     ]
    }
   ],
   "source": [
    "print('freq')\n",
    "train_freq, test_freq = compute_question_freq(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features construction\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n"
     ]
    }
   ],
   "source": [
    "lda_df= pd.read_csv('lda_df.csv')\n",
    "df_tags = pd.read_csv('df_tags.csv')\n",
    "\n",
    "print('Features construction')\n",
    "X_train = construct_data(pairs_train, A, lda_df, df_tags, train_freq)\n",
    "X_test = construct_data(pairs_test, A, lda_df, df_tags, test_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "pd.DataFrame(X_train).to_csv('X_train_processed.csv', index=False, header=True)\n",
    "pd.DataFrame(X_test).to_csv('X_test_processed.csv', index=False, header=True)\n",
    "pd.DataFrame(y_train).to_csv('y_train_processed.csv', index=False, header=False)\n",
    "pd.DataFrame(y_true).to_csv('y_test_processed.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We explore our data\n",
    "\n",
    "Si on doit supprimer des features, on le fait ici comme ça on recalcule pas tout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cosine similarity (CS)</th>\n",
       "      <th>total length (TL)</th>\n",
       "      <th>difference length (DL)</th>\n",
       "      <th>sim pos tags (POS)</th>\n",
       "      <th>POS*DL</th>\n",
       "      <th>POS*TL</th>\n",
       "      <th>POS*POS*TL</th>\n",
       "      <th>POS*POS*DL</th>\n",
       "      <th>common words (CW)</th>\n",
       "      <th>CW*POS</th>\n",
       "      <th>...</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.648169</td>\n",
       "      <td>31.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>2.142857</td>\n",
       "      <td>13.285714</td>\n",
       "      <td>5.693878</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.798655</td>\n",
       "      <td>35.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>11.666667</td>\n",
       "      <td>3.888889</td>\n",
       "      <td>1.444444</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.416006</td>\n",
       "      <td>51.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>1.222222</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.135802</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.588235</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.705351</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>10.333333</td>\n",
       "      <td>3.444444</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cosine similarity (CS)  total length (TL)  difference length (DL)  \\\n",
       "0                0.648169               31.0                     5.0   \n",
       "1                1.000000               22.0                     0.0   \n",
       "2                0.798655               35.0                    13.0   \n",
       "3                0.416006               51.0                    11.0   \n",
       "4                0.705351               31.0                     1.0   \n",
       "\n",
       "   sim pos tags (POS)    POS*DL     POS*TL  POS*POS*TL  POS*POS*DL  \\\n",
       "0            0.428571  2.142857  13.285714    5.693878    0.918367   \n",
       "1            0.000000  0.000000   0.000000    0.000000    0.000000   \n",
       "2            0.333333  4.333333  11.666667    3.888889    1.444444   \n",
       "3            0.111111  1.222222   5.666667    0.629630    0.135802   \n",
       "4            0.333333  0.333333  10.333333    3.444444    0.111111   \n",
       "\n",
       "   common words (CW)    CW*POS  ...     1     2    3     4    5     1    2  \\\n",
       "0                7.0  3.000000  ...   1.0  0.75  1.0  0.75  1.0  0.75  0.0   \n",
       "1               10.0  0.000000  ...   0.0  1.00  0.0  1.00  0.0  1.00  1.0   \n",
       "2                8.0  2.666667  ...   2.0  0.50  2.0  0.50  2.0  0.50  0.0   \n",
       "3                9.0  1.000000  ...   1.0  1.25  1.0  1.25  1.0  1.25  0.0   \n",
       "4                8.0  2.666667  ...   0.0  1.00  0.0  1.00  0.0  1.00  1.0   \n",
       "\n",
       "      3         4     5  \n",
       "0   4.0  0.733333   4.0  \n",
       "1   0.0  1.000000   0.0  \n",
       "2  11.0  0.476190  11.0  \n",
       "3  10.0  1.588235  10.0  \n",
       "4   1.0  0.928571   1.0  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X_train = pd.DataFrame(X_train, columns = features)\n",
    "df_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cosine similarity (CS)</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.597075</td>\n",
       "      <td>0.228375</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.439604</td>\n",
       "      <td>0.608753</td>\n",
       "      <td>0.767671</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total length (TL)</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>58.328500</td>\n",
       "      <td>25.440741</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>233.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>difference length (DL)</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>8.778794</td>\n",
       "      <td>9.497150</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>146.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sim pos tags (POS)</th>\n",
       "      <td>56067.0</td>\n",
       "      <td>0.258185</td>\n",
       "      <td>0.184302</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POS*DL</th>\n",
       "      <td>56067.0</td>\n",
       "      <td>2.981948</td>\n",
       "      <td>4.816185</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>100.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POS*TL</th>\n",
       "      <td>56067.0</td>\n",
       "      <td>14.939962</td>\n",
       "      <td>12.324250</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.857143</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>140.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POS*POS*TL</th>\n",
       "      <td>56067.0</td>\n",
       "      <td>5.627731</td>\n",
       "      <td>7.202605</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>3.111111</td>\n",
       "      <td>7.714286</td>\n",
       "      <td>99.049887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POS*POS*DL</th>\n",
       "      <td>56067.0</td>\n",
       "      <td>1.268223</td>\n",
       "      <td>2.857248</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.057851</td>\n",
       "      <td>0.297521</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>69.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>common words (CW)</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>12.266827</td>\n",
       "      <td>3.156652</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CW*POS</th>\n",
       "      <td>56067.0</td>\n",
       "      <td>3.032283</td>\n",
       "      <td>2.132253</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CW*POS*DL</th>\n",
       "      <td>56067.0</td>\n",
       "      <td>36.105109</td>\n",
       "      <td>60.917782</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>1505.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CW*POS*TL</th>\n",
       "      <td>56067.0</td>\n",
       "      <td>190.374733</td>\n",
       "      <td>181.182872</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>66.181818</td>\n",
       "      <td>149.500000</td>\n",
       "      <td>263.076923</td>\n",
       "      <td>2103.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuzz ratio</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.726384</td>\n",
       "      <td>0.142044</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuzz partial_ratio</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.778511</td>\n",
       "      <td>0.128986</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuzz token_sort_ratio</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.685835</td>\n",
       "      <td>0.162795</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuzz token_set_ratio</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.820691</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuzz partial_token_sort_ratio</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.743432</td>\n",
       "      <td>0.161921</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaro_distance</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.743938</td>\n",
       "      <td>0.122790</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.662068</td>\n",
       "      <td>0.732872</td>\n",
       "      <td>0.825590</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaro_winkler</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.766652</td>\n",
       "      <td>0.138084</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.662068</td>\n",
       "      <td>0.748074</td>\n",
       "      <td>0.889464</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_index n=2</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.522023</td>\n",
       "      <td>0.213772</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_index_norm_a n=2</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.680004</td>\n",
       "      <td>0.219870</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_index_norm_b n=2</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.682171</td>\n",
       "      <td>0.220239</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_index n=3</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.448878</td>\n",
       "      <td>0.224807</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_index_norm_a n=3</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.608944</td>\n",
       "      <td>0.241334</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_index_norm_b n=3</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.611135</td>\n",
       "      <td>0.242984</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_index n=4</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.401379</td>\n",
       "      <td>0.231375</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.229885</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_index_norm_a n=4</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.558218</td>\n",
       "      <td>0.253676</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_index_norm_b n=4</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.560465</td>\n",
       "      <td>0.256274</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_index n=5</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.358269</td>\n",
       "      <td>0.239941</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.309091</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_index_norm_a n=5</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.507605</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>0.697498</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_tag_cosine</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.109424</td>\n",
       "      <td>0.117075</td>\n",
       "      <td>-2.220446e-16</td>\n",
       "      <td>0.029857</td>\n",
       "      <td>0.074180</td>\n",
       "      <td>0.151472</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_tag_euclidean</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>2.001982</td>\n",
       "      <td>1.369286</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>17.492856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_tag_euclidean</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.438101</td>\n",
       "      <td>0.636135</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_tag_count_diff</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.332352</td>\n",
       "      <td>0.586483</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diff_words</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>1.931978</td>\n",
       "      <td>1.748518</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda_1</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.300948</td>\n",
       "      <td>0.291283</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.067804</td>\n",
       "      <td>0.202257</td>\n",
       "      <td>0.452729</td>\n",
       "      <td>0.999300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda_2</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.368650</td>\n",
       "      <td>0.215309</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.237188</td>\n",
       "      <td>0.353552</td>\n",
       "      <td>0.479264</td>\n",
       "      <td>1.285526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>common_diff</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>17953.208988</td>\n",
       "      <td>43282.684940</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.499775</td>\n",
       "      <td>7.999200</td>\n",
       "      <td>13.998600</td>\n",
       "      <td>230000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>common_min</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>3.183659</td>\n",
       "      <td>0.921774</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.499938</td>\n",
       "      <td>2.999925</td>\n",
       "      <td>3.666544</td>\n",
       "      <td>8.999100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>common_max</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>2.436043</td>\n",
       "      <td>0.811258</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.874977</td>\n",
       "      <td>2.333294</td>\n",
       "      <td>2.799944</td>\n",
       "      <td>8.999100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_l</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>4.913358</td>\n",
       "      <td>2.033809</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>19.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>same_last</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.444623</td>\n",
       "      <td>0.496928</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>same_first</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.406866</td>\n",
       "      <td>0.491254</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq_q1</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>7.765614</td>\n",
       "      <td>8.110506</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>116.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq_q2</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>7.093098</td>\n",
       "      <td>9.835005</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>116.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.618956</td>\n",
       "      <td>0.213529</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>0.479787</td>\n",
       "      <td>0.228018</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>2.914072</td>\n",
       "      <td>1.481942</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>6.725557</td>\n",
       "      <td>3.036212</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>6.683717</td>\n",
       "      <td>3.036765</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>1.362939</td>\n",
       "      <td>1.563909</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56057.0</td>\n",
       "      <td>1.080194</td>\n",
       "      <td>0.471626</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>1.277510</td>\n",
       "      <td>1.424805</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56057.0</td>\n",
       "      <td>1.073589</td>\n",
       "      <td>0.449104</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>1.260638</td>\n",
       "      <td>1.418690</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56057.0</td>\n",
       "      <td>1.072895</td>\n",
       "      <td>0.446146</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56051.0</td>\n",
       "      <td>0.407004</td>\n",
       "      <td>0.491280</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>7.451668</td>\n",
       "      <td>8.038137</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>126.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56057.0</td>\n",
       "      <td>1.084502</td>\n",
       "      <td>0.502204</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.263158</td>\n",
       "      <td>12.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56070.0</td>\n",
       "      <td>7.000874</td>\n",
       "      <td>7.336913</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>109.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 count          mean           std  \\\n",
       "cosine similarity (CS)         56070.0      0.597075      0.228375   \n",
       "total length (TL)              56070.0     58.328500     25.440741   \n",
       "difference length (DL)         56070.0      8.778794      9.497150   \n",
       "sim pos tags (POS)             56067.0      0.258185      0.184302   \n",
       "POS*DL                         56067.0      2.981948      4.816185   \n",
       "POS*TL                         56067.0     14.939962     12.324250   \n",
       "POS*POS*TL                     56067.0      5.627731      7.202605   \n",
       "POS*POS*DL                     56067.0      1.268223      2.857248   \n",
       "common words (CW)              56070.0     12.266827      3.156652   \n",
       "CW*POS                         56067.0      3.032283      2.132253   \n",
       "CW*POS*DL                      56067.0     36.105109     60.917782   \n",
       "CW*POS*TL                      56067.0    190.374733    181.182872   \n",
       "fuzz ratio                     56070.0      0.726384      0.142044   \n",
       "fuzz partial_ratio             56070.0      0.778511      0.128986   \n",
       "fuzz token_sort_ratio          56070.0      0.685835      0.162795   \n",
       "fuzz token_set_ratio           56070.0      0.820691      0.161900   \n",
       "fuzz partial_token_sort_ratio  56070.0      0.743432      0.161921   \n",
       "jaro_distance                  56070.0      0.743938      0.122790   \n",
       "jaro_winkler                   56070.0      0.766652      0.138084   \n",
       "jaccard_index n=2              56070.0      0.522023      0.213772   \n",
       "jaccard_index_norm_a n=2       56070.0      0.680004      0.219870   \n",
       "jaccard_index_norm_b n=2       56070.0      0.682171      0.220239   \n",
       "jaccard_index n=3              56070.0      0.448878      0.224807   \n",
       "jaccard_index_norm_a n=3       56070.0      0.608944      0.241334   \n",
       "jaccard_index_norm_b n=3       56070.0      0.611135      0.242984   \n",
       "jaccard_index n=4              56070.0      0.401379      0.231375   \n",
       "jaccard_index_norm_a n=4       56070.0      0.558218      0.253676   \n",
       "jaccard_index_norm_b n=4       56070.0      0.560465      0.256274   \n",
       "jaccard_index n=5              56070.0      0.358269      0.239941   \n",
       "jaccard_index_norm_a n=5       56070.0      0.507605      0.269071   \n",
       "...                                ...           ...           ...   \n",
       "pos_tag_cosine                 56070.0      0.109424      0.117075   \n",
       "pos_tag_euclidean              56070.0      2.001982      1.369286   \n",
       "ner_tag_euclidean              56070.0      0.438101      0.636135   \n",
       "ner_tag_count_diff             56070.0      0.332352      0.586483   \n",
       "diff_words                     56070.0      1.931978      1.748518   \n",
       "lda_1                          56070.0      0.300948      0.291283   \n",
       "lda_2                          56070.0      0.368650      0.215309   \n",
       "common_diff                    56070.0  17953.208988  43282.684940   \n",
       "common_min                     56070.0      3.183659      0.921774   \n",
       "common_max                     56070.0      2.436043      0.811258   \n",
       "mean_l                         56070.0      4.913358      2.033809   \n",
       "same_last                      56070.0      0.444623      0.496928   \n",
       "same_first                     56070.0      0.406866      0.491254   \n",
       "freq_q1                        56070.0      7.765614      8.110506   \n",
       "freq_q2                        56070.0      7.093098      9.835005   \n",
       "1                              56070.0      0.618956      0.213529   \n",
       "2                              56070.0      0.479787      0.228018   \n",
       "3                              56070.0      2.914072      1.481942   \n",
       "4                              56070.0      6.725557      3.036212   \n",
       "5                              56070.0      6.683717      3.036765   \n",
       "1                              56070.0      1.362939      1.563909   \n",
       "2                              56057.0      1.080194      0.471626   \n",
       "3                              56070.0      1.277510      1.424805   \n",
       "4                              56057.0      1.073589      0.449104   \n",
       "5                              56070.0      1.260638      1.418690   \n",
       "1                              56057.0      1.072895      0.446146   \n",
       "2                              56051.0      0.407004      0.491280   \n",
       "3                              56070.0      7.451668      8.038137   \n",
       "4                              56057.0      1.084502      0.502204   \n",
       "5                              56070.0      7.000874      7.336913   \n",
       "\n",
       "                                        min        25%         50%  \\\n",
       "cosine similarity (CS)         0.000000e+00   0.439604    0.608753   \n",
       "total length (TL)              0.000000e+00  40.000000   53.000000   \n",
       "difference length (DL)         0.000000e+00   2.000000    6.000000   \n",
       "sim pos tags (POS)             0.000000e+00   0.125000    0.250000   \n",
       "POS*DL                         0.000000e+00   0.363636    1.285714   \n",
       "POS*TL                         0.000000e+00   5.857143   12.500000   \n",
       "POS*POS*TL                     0.000000e+00   0.775510    3.111111   \n",
       "POS*POS*DL                     0.000000e+00   0.057851    0.297521   \n",
       "common words (CW)              0.000000e+00  10.000000   12.000000   \n",
       "CW*POS                         0.000000e+00   1.500000    2.800000   \n",
       "CW*POS*DL                      0.000000e+00   4.571429   14.285714   \n",
       "CW*POS*TL                      0.000000e+00  66.181818  149.500000   \n",
       "fuzz ratio                     0.000000e+00   0.620000    0.720000   \n",
       "fuzz partial_ratio             0.000000e+00   0.680000    0.770000   \n",
       "fuzz token_sort_ratio          0.000000e+00   0.580000    0.680000   \n",
       "fuzz token_set_ratio           0.000000e+00   0.720000    0.840000   \n",
       "fuzz partial_token_sort_ratio  0.000000e+00   0.630000    0.740000   \n",
       "jaro_distance                  0.000000e+00   0.662068    0.732872   \n",
       "jaro_winkler                   0.000000e+00   0.662068    0.748074   \n",
       "jaccard_index n=2              0.000000e+00   0.370370    0.500000   \n",
       "jaccard_index_norm_a n=2       0.000000e+00   0.533333    0.687500   \n",
       "jaccard_index_norm_b n=2       0.000000e+00   0.529412    0.687500   \n",
       "jaccard_index n=3              0.000000e+00   0.285714    0.421053   \n",
       "jaccard_index_norm_a n=3       0.000000e+00   0.440000    0.608696   \n",
       "jaccard_index_norm_b n=3       0.000000e+00   0.437500    0.608696   \n",
       "jaccard_index n=4              0.000000e+00   0.229885    0.363636   \n",
       "jaccard_index_norm_a n=4       0.000000e+00   0.375000    0.548387   \n",
       "jaccard_index_norm_b n=4       0.000000e+00   0.369565    0.545455   \n",
       "jaccard_index n=5              0.000000e+00   0.178571    0.309091   \n",
       "jaccard_index_norm_a n=5       0.000000e+00   0.304348    0.485714   \n",
       "...                                     ...        ...         ...   \n",
       "pos_tag_cosine                -2.220446e-16   0.029857    0.074180   \n",
       "pos_tag_euclidean              0.000000e+00   1.000000    1.732051   \n",
       "ner_tag_euclidean              0.000000e+00   0.000000    0.000000   \n",
       "ner_tag_count_diff             0.000000e+00   0.000000    0.000000   \n",
       "diff_words                     0.000000e+00   1.000000    1.000000   \n",
       "lda_1                          0.000000e+00   0.067804    0.202257   \n",
       "lda_2                          0.000000e+00   0.237188    0.353552   \n",
       "common_diff                    0.000000e+00   4.499775    7.999200   \n",
       "common_min                     0.000000e+00   2.499938    2.999925   \n",
       "common_max                     0.000000e+00   1.874977    2.333294   \n",
       "mean_l                         0.000000e+00   3.500000    4.500000   \n",
       "same_last                      0.000000e+00   0.000000    0.000000   \n",
       "same_first                     0.000000e+00   0.000000    0.000000   \n",
       "freq_q1                        1.000000e+00   3.000000    5.000000   \n",
       "freq_q2                        0.000000e+00   0.000000    4.000000   \n",
       "1                              0.000000e+00   0.500000    0.666667   \n",
       "2                              0.000000e+00   0.333333    0.461538   \n",
       "3                              0.000000e+00   2.000000    3.000000   \n",
       "4                              0.000000e+00   5.000000    6.000000   \n",
       "5                              0.000000e+00   5.000000    6.000000   \n",
       "1                              0.000000e+00   0.000000    1.000000   \n",
       "2                              0.000000e+00   0.800000    1.000000   \n",
       "3                              0.000000e+00   0.000000    1.000000   \n",
       "4                              0.000000e+00   0.800000    1.000000   \n",
       "5                              0.000000e+00   0.000000    1.000000   \n",
       "1                              0.000000e+00   0.800000    1.000000   \n",
       "2                              0.000000e+00   0.000000    0.000000   \n",
       "3                              0.000000e+00   2.000000    5.000000   \n",
       "4                              0.000000e+00   0.785714    1.000000   \n",
       "5                              0.000000e+00   2.000000    5.000000   \n",
       "\n",
       "                                      75%            max  \n",
       "cosine similarity (CS)           0.767671       1.000000  \n",
       "total length (TL)               71.000000     233.000000  \n",
       "difference length (DL)          12.000000     146.000000  \n",
       "sim pos tags (POS)               0.333333       1.000000  \n",
       "POS*DL                           3.600000     100.375000  \n",
       "POS*TL                          21.000000     140.250000  \n",
       "POS*POS*TL                       7.714286      99.049887  \n",
       "POS*POS*DL                       1.250000      69.007812  \n",
       "common words (CW)               14.000000      24.000000  \n",
       "CW*POS                           4.333333      15.000000  \n",
       "CW*POS*DL                       42.000000    1505.625000  \n",
       "CW*POS*TL                      263.076923    2103.750000  \n",
       "fuzz ratio                       0.830000       1.000000  \n",
       "fuzz partial_ratio               0.880000       1.000000  \n",
       "fuzz token_sort_ratio            0.800000       1.000000  \n",
       "fuzz token_set_ratio             1.000000       1.000000  \n",
       "fuzz partial_token_sort_ratio    0.850000       1.000000  \n",
       "jaro_distance                    0.825590       1.000000  \n",
       "jaro_winkler                     0.889464       1.000000  \n",
       "jaccard_index n=2                0.666667       1.000000  \n",
       "jaccard_index_norm_a n=2         0.843750       1.000000  \n",
       "jaccard_index_norm_b n=2         0.850000       1.000000  \n",
       "jaccard_index n=3                0.592593       1.000000  \n",
       "jaccard_index_norm_a n=3         0.785714       1.000000  \n",
       "jaccard_index_norm_b n=3         0.793103       1.000000  \n",
       "jaccard_index n=4                0.540541       1.000000  \n",
       "jaccard_index_norm_a n=4         0.741935       1.000000  \n",
       "jaccard_index_norm_b n=4         0.750000       1.000000  \n",
       "jaccard_index n=5                0.500000       1.000000  \n",
       "jaccard_index_norm_a n=5         0.697498       1.000000  \n",
       "...                                   ...            ...  \n",
       "pos_tag_cosine                   0.151472       1.000000  \n",
       "pos_tag_euclidean                2.645751      17.492856  \n",
       "ner_tag_euclidean                1.000000       5.000000  \n",
       "ner_tag_count_diff               1.000000       6.000000  \n",
       "diff_words                       3.000000      20.000000  \n",
       "lda_1                            0.452729       0.999300  \n",
       "lda_2                            0.479264       1.285526  \n",
       "common_diff                     13.998600  230000.000000  \n",
       "common_min                       3.666544       8.999100  \n",
       "common_max                       2.799944       8.999100  \n",
       "mean_l                           6.000000      19.500000  \n",
       "same_last                        1.000000       1.000000  \n",
       "same_first                       1.000000       1.000000  \n",
       "freq_q1                         10.000000     116.000000  \n",
       "freq_q2                         10.000000     116.000000  \n",
       "1                                0.800000       1.000000  \n",
       "2                                0.625000       1.000000  \n",
       "3                                4.000000      16.000000  \n",
       "4                                8.000000      30.000000  \n",
       "5                                8.000000      30.000000  \n",
       "1                                2.000000      20.000000  \n",
       "2                                1.250000      10.000000  \n",
       "3                                2.000000      18.000000  \n",
       "4                                1.250000      10.000000  \n",
       "5                                2.000000      18.000000  \n",
       "1                                1.250000      10.000000  \n",
       "2                                1.000000       1.000000  \n",
       "3                               10.000000     126.000000  \n",
       "4                                1.263158      12.250000  \n",
       "5                               10.000000     109.000000  \n",
       "\n",
       "[61 rows x 8 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X_train.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cosine similarity (CS)</th>\n",
       "      <th>total length (TL)</th>\n",
       "      <th>difference length (DL)</th>\n",
       "      <th>sim pos tags (POS)</th>\n",
       "      <th>POS*DL</th>\n",
       "      <th>POS*TL</th>\n",
       "      <th>POS*POS*TL</th>\n",
       "      <th>POS*POS*DL</th>\n",
       "      <th>common words (CW)</th>\n",
       "      <th>CW*POS</th>\n",
       "      <th>...</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.505996</td>\n",
       "      <td>116.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>38.666667</td>\n",
       "      <td>12.888889</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.325581</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.626199</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.041667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.543760</td>\n",
       "      <td>34.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>12.240000</td>\n",
       "      <td>1.440000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.214286</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.788589</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>13.200000</td>\n",
       "      <td>2.640000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.453056</td>\n",
       "      <td>65.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>10.833333</td>\n",
       "      <td>1.805556</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.153846</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cosine similarity (CS)  total length (TL)  difference length (DL)  \\\n",
       "0                0.505996              116.0                    16.0   \n",
       "1                0.626199               57.0                     1.0   \n",
       "2                0.543760               34.0                     4.0   \n",
       "3                0.788589               66.0                     2.0   \n",
       "4                0.453056               65.0                     5.0   \n",
       "\n",
       "   sim pos tags (POS)    POS*DL     POS*TL  POS*POS*TL  POS*POS*DL  \\\n",
       "0            0.333333  5.333333  38.666667   12.888889    1.777778   \n",
       "1            0.000000  0.000000   0.000000    0.000000    0.000000   \n",
       "2            0.600000  2.400000  20.400000   12.240000    1.440000   \n",
       "3            0.200000  0.400000  13.200000    2.640000    0.080000   \n",
       "4            0.166667  0.833333  10.833333    1.805556    0.138889   \n",
       "\n",
       "   common words (CW)    CW*POS  ...     1     2    3     4    5     1    2  \\\n",
       "0               16.0  5.333333  ...   2.0  1.25  2.0  1.25  2.0  1.25  0.0   \n",
       "1               14.0  0.000000  ...   0.0  1.00  0.0  1.00  0.0  1.00  0.0   \n",
       "2                9.0  5.400000  ...   1.0  1.50  1.0  1.50  1.0  1.50  0.0   \n",
       "3               15.0  3.000000  ...   0.0  1.00  0.0  1.00  0.0  1.00  1.0   \n",
       "4               13.0  2.166667  ...   1.0  1.20  1.0  1.20  1.0  1.20  0.0   \n",
       "\n",
       "      3         4     5  \n",
       "0  14.0  1.325581  14.0  \n",
       "1   1.0  1.041667   1.0  \n",
       "2   3.0  1.214286   3.0  \n",
       "3   2.0  1.071429   2.0  \n",
       "4   4.0  1.153846   4.0  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X_test = pd.DataFrame(X_test, columns = features)\n",
    "df_X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cosine similarity (CS)</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>0.595348</td>\n",
       "      <td>0.228124</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.441623</td>\n",
       "      <td>0.608951</td>\n",
       "      <td>0.764798</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total length (TL)</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>58.411319</td>\n",
       "      <td>25.435553</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>204.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>difference length (DL)</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>8.765959</td>\n",
       "      <td>9.452334</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>105.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sim pos tags (POS)</th>\n",
       "      <td>24029.0</td>\n",
       "      <td>0.260211</td>\n",
       "      <td>0.184285</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POS*DL</th>\n",
       "      <td>24029.0</td>\n",
       "      <td>2.992384</td>\n",
       "      <td>4.852433</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>73.913043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POS*TL</th>\n",
       "      <td>24029.0</td>\n",
       "      <td>15.042624</td>\n",
       "      <td>12.267044</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.909091</td>\n",
       "      <td>12.555556</td>\n",
       "      <td>21.230769</td>\n",
       "      <td>108.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POS*POS*TL</th>\n",
       "      <td>24029.0</td>\n",
       "      <td>5.681885</td>\n",
       "      <td>7.214705</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>7.888889</td>\n",
       "      <td>77.854671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POS*POS*DL</th>\n",
       "      <td>24029.0</td>\n",
       "      <td>1.282170</td>\n",
       "      <td>2.934638</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.059172</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>1.264463</td>\n",
       "      <td>54.631380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>common words (CW)</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>12.262256</td>\n",
       "      <td>3.155884</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CW*POS</th>\n",
       "      <td>24029.0</td>\n",
       "      <td>3.052218</td>\n",
       "      <td>2.126148</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.555556</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>4.363636</td>\n",
       "      <td>13.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CW*POS*DL</th>\n",
       "      <td>24029.0</td>\n",
       "      <td>36.097778</td>\n",
       "      <td>60.788461</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.615385</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>943.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CW*POS*TL</th>\n",
       "      <td>24029.0</td>\n",
       "      <td>191.279090</td>\n",
       "      <td>179.412849</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>67.846154</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>265.909091</td>\n",
       "      <td>1958.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuzz ratio</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>0.725411</td>\n",
       "      <td>0.141908</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuzz partial_ratio</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>0.777003</td>\n",
       "      <td>0.129786</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuzz token_sort_ratio</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>0.684112</td>\n",
       "      <td>0.162795</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuzz token_set_ratio</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>0.818533</td>\n",
       "      <td>0.163982</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuzz partial_token_sort_ratio</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>0.740996</td>\n",
       "      <td>0.161703</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaro_distance</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>0.742459</td>\n",
       "      <td>0.122861</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.661492</td>\n",
       "      <td>0.732661</td>\n",
       "      <td>0.824362</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaro_winkler</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>0.765286</td>\n",
       "      <td>0.138452</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.661492</td>\n",
       "      <td>0.747241</td>\n",
       "      <td>0.888370</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_index n=2</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>0.519442</td>\n",
       "      <td>0.212362</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_index_norm_a n=2</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>0.675879</td>\n",
       "      <td>0.220902</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_index_norm_b n=2</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>0.681581</td>\n",
       "      <td>0.219451</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_index n=3</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>0.445736</td>\n",
       "      <td>0.222996</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_index_norm_a n=3</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>0.604280</td>\n",
       "      <td>0.242544</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_index_norm_b n=3</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>0.609704</td>\n",
       "      <td>0.242796</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_index n=4</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>0.398274</td>\n",
       "      <td>0.229031</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_index_norm_a n=4</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>0.553647</td>\n",
       "      <td>0.254688</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_index_norm_b n=4</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>0.558950</td>\n",
       "      <td>0.255873</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_index n=5</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>0.355161</td>\n",
       "      <td>0.237119</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_index_norm_a n=5</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>0.503161</td>\n",
       "      <td>0.269664</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_tag_cosine</th>\n",
       "      <td>24029.0</td>\n",
       "      <td>0.109377</td>\n",
       "      <td>0.116252</td>\n",
       "      <td>-2.220446e-16</td>\n",
       "      <td>0.029857</td>\n",
       "      <td>0.074180</td>\n",
       "      <td>0.151472</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_tag_euclidean</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>2.005151</td>\n",
       "      <td>1.391418</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>28.035692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_tag_euclidean</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>0.439537</td>\n",
       "      <td>0.641049</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.099020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_tag_count_diff</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>0.334041</td>\n",
       "      <td>0.592434</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diff_words</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>1.957553</td>\n",
       "      <td>1.763454</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda_1</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>0.302160</td>\n",
       "      <td>0.291940</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.068080</td>\n",
       "      <td>0.203658</td>\n",
       "      <td>0.452838</td>\n",
       "      <td>0.999260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lda_2</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>0.370011</td>\n",
       "      <td>0.216015</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.240108</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.478634</td>\n",
       "      <td>1.279236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>common_diff</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>17600.357161</td>\n",
       "      <td>43100.413274</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.333189</td>\n",
       "      <td>7.999200</td>\n",
       "      <td>13.998600</td>\n",
       "      <td>230000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>common_min</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>3.168442</td>\n",
       "      <td>0.912302</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.499938</td>\n",
       "      <td>2.999925</td>\n",
       "      <td>3.666544</td>\n",
       "      <td>9.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>common_max</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>2.428151</td>\n",
       "      <td>0.803846</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.874977</td>\n",
       "      <td>2.333294</td>\n",
       "      <td>2.799944</td>\n",
       "      <td>9.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_l</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>4.923450</td>\n",
       "      <td>2.031397</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>16.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>same_last</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>0.441698</td>\n",
       "      <td>0.496600</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>same_first</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>0.404161</td>\n",
       "      <td>0.490739</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq_q1</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>7.737578</td>\n",
       "      <td>8.161682</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>116.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq_q2</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>7.034374</td>\n",
       "      <td>9.913722</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>116.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>0.616354</td>\n",
       "      <td>0.214371</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>0.477089</td>\n",
       "      <td>0.226862</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>2.908240</td>\n",
       "      <td>1.481230</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>6.748689</td>\n",
       "      <td>3.044056</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>6.707324</td>\n",
       "      <td>3.044119</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>1.357012</td>\n",
       "      <td>1.566512</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24022.0</td>\n",
       "      <td>1.073553</td>\n",
       "      <td>0.474623</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>1.271328</td>\n",
       "      <td>1.417167</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24022.0</td>\n",
       "      <td>1.066818</td>\n",
       "      <td>0.447559</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>1.258593</td>\n",
       "      <td>1.410576</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24022.0</td>\n",
       "      <td>1.065720</td>\n",
       "      <td>0.445099</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24017.0</td>\n",
       "      <td>0.404380</td>\n",
       "      <td>0.490782</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>7.443820</td>\n",
       "      <td>7.995639</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>89.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24022.0</td>\n",
       "      <td>1.077636</td>\n",
       "      <td>0.501020</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>9.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24030.0</td>\n",
       "      <td>6.984145</td>\n",
       "      <td>7.258395</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>77.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 count          mean           std  \\\n",
       "cosine similarity (CS)         24030.0      0.595348      0.228124   \n",
       "total length (TL)              24030.0     58.411319     25.435553   \n",
       "difference length (DL)         24030.0      8.765959      9.452334   \n",
       "sim pos tags (POS)             24029.0      0.260211      0.184285   \n",
       "POS*DL                         24029.0      2.992384      4.852433   \n",
       "POS*TL                         24029.0     15.042624     12.267044   \n",
       "POS*POS*TL                     24029.0      5.681885      7.214705   \n",
       "POS*POS*DL                     24029.0      1.282170      2.934638   \n",
       "common words (CW)              24030.0     12.262256      3.155884   \n",
       "CW*POS                         24029.0      3.052218      2.126148   \n",
       "CW*POS*DL                      24029.0     36.097778     60.788461   \n",
       "CW*POS*TL                      24029.0    191.279090    179.412849   \n",
       "fuzz ratio                     24030.0      0.725411      0.141908   \n",
       "fuzz partial_ratio             24030.0      0.777003      0.129786   \n",
       "fuzz token_sort_ratio          24030.0      0.684112      0.162795   \n",
       "fuzz token_set_ratio           24030.0      0.818533      0.163982   \n",
       "fuzz partial_token_sort_ratio  24030.0      0.740996      0.161703   \n",
       "jaro_distance                  24030.0      0.742459      0.122861   \n",
       "jaro_winkler                   24030.0      0.765286      0.138452   \n",
       "jaccard_index n=2              24030.0      0.519442      0.212362   \n",
       "jaccard_index_norm_a n=2       24030.0      0.675879      0.220902   \n",
       "jaccard_index_norm_b n=2       24030.0      0.681581      0.219451   \n",
       "jaccard_index n=3              24030.0      0.445736      0.222996   \n",
       "jaccard_index_norm_a n=3       24030.0      0.604280      0.242544   \n",
       "jaccard_index_norm_b n=3       24030.0      0.609704      0.242796   \n",
       "jaccard_index n=4              24030.0      0.398274      0.229031   \n",
       "jaccard_index_norm_a n=4       24030.0      0.553647      0.254688   \n",
       "jaccard_index_norm_b n=4       24030.0      0.558950      0.255873   \n",
       "jaccard_index n=5              24030.0      0.355161      0.237119   \n",
       "jaccard_index_norm_a n=5       24030.0      0.503161      0.269664   \n",
       "...                                ...           ...           ...   \n",
       "pos_tag_cosine                 24029.0      0.109377      0.116252   \n",
       "pos_tag_euclidean              24030.0      2.005151      1.391418   \n",
       "ner_tag_euclidean              24030.0      0.439537      0.641049   \n",
       "ner_tag_count_diff             24030.0      0.334041      0.592434   \n",
       "diff_words                     24030.0      1.957553      1.763454   \n",
       "lda_1                          24030.0      0.302160      0.291940   \n",
       "lda_2                          24030.0      0.370011      0.216015   \n",
       "common_diff                    24030.0  17600.357161  43100.413274   \n",
       "common_min                     24030.0      3.168442      0.912302   \n",
       "common_max                     24030.0      2.428151      0.803846   \n",
       "mean_l                         24030.0      4.923450      2.031397   \n",
       "same_last                      24030.0      0.441698      0.496600   \n",
       "same_first                     24030.0      0.404161      0.490739   \n",
       "freq_q1                        24030.0      7.737578      8.161682   \n",
       "freq_q2                        24030.0      7.034374      9.913722   \n",
       "1                              24030.0      0.616354      0.214371   \n",
       "2                              24030.0      0.477089      0.226862   \n",
       "3                              24030.0      2.908240      1.481230   \n",
       "4                              24030.0      6.748689      3.044056   \n",
       "5                              24030.0      6.707324      3.044119   \n",
       "1                              24030.0      1.357012      1.566512   \n",
       "2                              24022.0      1.073553      0.474623   \n",
       "3                              24030.0      1.271328      1.417167   \n",
       "4                              24022.0      1.066818      0.447559   \n",
       "5                              24030.0      1.258593      1.410576   \n",
       "1                              24022.0      1.065720      0.445099   \n",
       "2                              24017.0      0.404380      0.490782   \n",
       "3                              24030.0      7.443820      7.995639   \n",
       "4                              24022.0      1.077636      0.501020   \n",
       "5                              24030.0      6.984145      7.258395   \n",
       "\n",
       "                                        min        25%         50%  \\\n",
       "cosine similarity (CS)         0.000000e+00   0.441623    0.608951   \n",
       "total length (TL)              0.000000e+00  40.000000   53.000000   \n",
       "difference length (DL)         0.000000e+00   2.000000    6.000000   \n",
       "sim pos tags (POS)             0.000000e+00   0.142857    0.250000   \n",
       "POS*DL                         0.000000e+00   0.363636    1.285714   \n",
       "POS*TL                         0.000000e+00   5.909091   12.555556   \n",
       "POS*POS*TL                     0.000000e+00   0.795918    3.125000   \n",
       "POS*POS*DL                     0.000000e+00   0.059172    0.305556   \n",
       "common words (CW)              0.000000e+00  10.000000   12.000000   \n",
       "CW*POS                         0.000000e+00   1.555556    2.833333   \n",
       "CW*POS*DL                      0.000000e+00   4.615385   14.285714   \n",
       "CW*POS*TL                      0.000000e+00  67.846154  150.000000   \n",
       "fuzz ratio                     0.000000e+00   0.620000    0.720000   \n",
       "fuzz partial_ratio             0.000000e+00   0.680000    0.770000   \n",
       "fuzz token_sort_ratio          0.000000e+00   0.570000    0.680000   \n",
       "fuzz token_set_ratio           0.000000e+00   0.720000    0.840000   \n",
       "fuzz partial_token_sort_ratio  0.000000e+00   0.630000    0.740000   \n",
       "jaro_distance                  0.000000e+00   0.661492    0.732661   \n",
       "jaro_winkler                   0.000000e+00   0.661492    0.747241   \n",
       "jaccard_index n=2              0.000000e+00   0.369565    0.500000   \n",
       "jaccard_index_norm_a n=2       0.000000e+00   0.529412    0.687500   \n",
       "jaccard_index_norm_b n=2       0.000000e+00   0.533333    0.687500   \n",
       "jaccard_index n=3              0.000000e+00   0.282051    0.419355   \n",
       "jaccard_index_norm_a n=3       0.000000e+00   0.434783    0.600000   \n",
       "jaccard_index_norm_b n=3       0.000000e+00   0.434783    0.608696   \n",
       "jaccard_index n=4              0.000000e+00   0.227273    0.361111   \n",
       "jaccard_index_norm_a n=4       0.000000e+00   0.368421    0.541667   \n",
       "jaccard_index_norm_b n=4       0.000000e+00   0.368421    0.550000   \n",
       "jaccard_index n=5              0.000000e+00   0.177778    0.307692   \n",
       "jaccard_index_norm_a n=5       0.000000e+00   0.300000    0.478261   \n",
       "...                                     ...        ...         ...   \n",
       "pos_tag_cosine                -2.220446e-16   0.029857    0.074180   \n",
       "pos_tag_euclidean              0.000000e+00   1.000000    1.732051   \n",
       "ner_tag_euclidean              0.000000e+00   0.000000    0.000000   \n",
       "ner_tag_count_diff             0.000000e+00   0.000000    0.000000   \n",
       "diff_words                     0.000000e+00   1.000000    1.000000   \n",
       "lda_1                          0.000000e+00   0.068080    0.203658   \n",
       "lda_2                          0.000000e+00   0.240108    0.353553   \n",
       "common_diff                    0.000000e+00   4.333189    7.999200   \n",
       "common_min                     0.000000e+00   2.499938    2.999925   \n",
       "common_max                     0.000000e+00   1.874977    2.333294   \n",
       "mean_l                         0.000000e+00   3.500000    4.500000   \n",
       "same_last                      0.000000e+00   0.000000    0.000000   \n",
       "same_first                     0.000000e+00   0.000000    0.000000   \n",
       "freq_q1                        1.000000e+00   3.000000    5.000000   \n",
       "freq_q2                        0.000000e+00   0.000000    4.000000   \n",
       "1                              0.000000e+00   0.500000    0.666667   \n",
       "2                              0.000000e+00   0.333333    0.461538   \n",
       "3                              0.000000e+00   2.000000    3.000000   \n",
       "4                              0.000000e+00   5.000000    6.000000   \n",
       "5                              0.000000e+00   5.000000    6.000000   \n",
       "1                              0.000000e+00   0.000000    1.000000   \n",
       "2                              0.000000e+00   0.800000    1.000000   \n",
       "3                              0.000000e+00   0.000000    1.000000   \n",
       "4                              0.000000e+00   0.800000    1.000000   \n",
       "5                              0.000000e+00   0.000000    1.000000   \n",
       "1                              0.000000e+00   0.800000    1.000000   \n",
       "2                              0.000000e+00   0.000000    0.000000   \n",
       "3                              0.000000e+00   2.000000    5.000000   \n",
       "4                              0.000000e+00   0.777778    1.000000   \n",
       "5                              0.000000e+00   2.000000    5.000000   \n",
       "\n",
       "                                      75%            max  \n",
       "cosine similarity (CS)           0.764798       1.000000  \n",
       "total length (TL)               71.000000     204.000000  \n",
       "difference length (DL)          12.000000     105.000000  \n",
       "sim pos tags (POS)               0.333333       1.000000  \n",
       "POS*DL                           3.666667      73.913043  \n",
       "POS*TL                          21.230769     108.818182  \n",
       "POS*POS*TL                       7.888889      77.854671  \n",
       "POS*POS*DL                       1.264463      54.631380  \n",
       "common words (CW)               14.000000      25.000000  \n",
       "CW*POS                           4.363636      13.538462  \n",
       "CW*POS*DL                       42.000000     943.090909  \n",
       "CW*POS*TL                      265.909091    1958.727273  \n",
       "fuzz ratio                       0.830000       1.000000  \n",
       "fuzz partial_ratio               0.880000       1.000000  \n",
       "fuzz token_sort_ratio            0.800000       1.000000  \n",
       "fuzz token_set_ratio             1.000000       1.000000  \n",
       "fuzz partial_token_sort_ratio    0.850000       1.000000  \n",
       "jaro_distance                    0.824362       1.000000  \n",
       "jaro_winkler                     0.888370       1.000000  \n",
       "jaccard_index n=2                0.666667       1.000000  \n",
       "jaccard_index_norm_a n=2         0.840000       1.000000  \n",
       "jaccard_index_norm_b n=2         0.846154       1.000000  \n",
       "jaccard_index n=3                0.590909       1.000000  \n",
       "jaccard_index_norm_a n=3         0.782609       1.000000  \n",
       "jaccard_index_norm_b n=3         0.791667       1.000000  \n",
       "jaccard_index n=4                0.538462       1.000000  \n",
       "jaccard_index_norm_a n=4         0.739130       1.000000  \n",
       "jaccard_index_norm_b n=4         0.750000       1.000000  \n",
       "jaccard_index n=5                0.500000       1.000000  \n",
       "jaccard_index_norm_a n=5         0.695652       1.000000  \n",
       "...                                   ...            ...  \n",
       "pos_tag_cosine                   0.151472       1.000000  \n",
       "pos_tag_euclidean                2.645751      28.035692  \n",
       "ner_tag_euclidean                1.000000       5.099020  \n",
       "ner_tag_count_diff               1.000000       6.000000  \n",
       "diff_words                       3.000000      20.000000  \n",
       "lda_1                            0.452838       0.999260  \n",
       "lda_2                            0.478634       1.279236  \n",
       "common_diff                     13.998600  230000.000000  \n",
       "common_min                       3.666544       9.999000  \n",
       "common_max                       2.799944       9.999000  \n",
       "mean_l                           6.000000      16.500000  \n",
       "same_last                        1.000000       1.000000  \n",
       "same_first                       1.000000       1.000000  \n",
       "freq_q1                         10.000000     116.000000  \n",
       "freq_q2                         10.000000     116.000000  \n",
       "1                                0.800000       1.000000  \n",
       "2                                0.625000       1.000000  \n",
       "3                                4.000000      12.000000  \n",
       "4                                8.000000      27.000000  \n",
       "5                                8.000000      26.000000  \n",
       "1                                2.000000      17.000000  \n",
       "2                                1.250000      12.000000  \n",
       "3                                2.000000      15.000000  \n",
       "4                                1.250000      12.000000  \n",
       "5                                2.000000      15.000000  \n",
       "1                                1.250000      12.000000  \n",
       "2                                1.000000       1.000000  \n",
       "3                               10.000000      89.000000  \n",
       "4                                1.250000       9.800000  \n",
       "5                               10.000000      77.000000  \n",
       "\n",
       "[61 rows x 8 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X_test.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_normalize_all_features(df):\n",
    "    return (df-df.mean())/df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def minmax_normalize_all_features(df):\n",
    "    return (df-df.min())/(df.max()-df.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_X_train_mean_norm = mean_normalize_all_features(df_X_train.copy())\n",
    "df_X_test_mean_norm = mean_normalize_all_features(df_X_test.copy())\n",
    "\n",
    "df_X_train_minmax_norm = minmax_normalize_all_features(df_X_train.copy())\n",
    "df_X_test_minmax_norm = minmax_normalize_all_features(df_X_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def del_features(matrice, index):\n",
    "    for i in index:\n",
    "        matrice = np.delete(matrice,i,axis=1)\n",
    "    return matrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier\n",
    "\n",
    "(tester plrs modèles mais xgboost devrait etre le meilleur)\n",
    "\n",
    "sinon voir pour réseaux de neurones (modèles siamois, cnn, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.svm import SVC\n",
    "# Scores\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "clf = SVC(kernel = 'rbf', gamma = 1e-3, C = 1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(\"accuracy =\" + str(acc))\n",
    "loss = log_loss(y_true, y_pred, eps = 1e-15, normalize = True, sample_weight = None, labels = None)\n",
    "print(\"cross entropy =\" + str(loss)) # the mean loss per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train= pd.read_csv('X_train_processed.csv')\n",
    "#X_test = pd.read_csv('X_test_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xgboost \n",
    "\n",
    "http://xgboost.readthedocs.io/en/latest/python/python_intro.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy = 0.32702802205\n"
     ]
    }
   ],
   "source": [
    "def threshold(y_pred):\n",
    "    return [1 if y >= 0.5 else 0 for y in y_pred]\n",
    "\n",
    "# read in data\n",
    "dtrain = xgb.DMatrix(X_train, label = y_train)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "#### voir pour un grid search\n",
    "# specify parameters via map\n",
    "param = {'max_depth': 6, 'eta':0.1, 'silent':1, \"objective\" : \"binary:logistic\", \"eval_metric\": \"logloss\"}\n",
    "num_round = 309\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "\n",
    "# make prediction\n",
    "y_pred = bst.predict(dtest)\n",
    "#print(y_pred)\n",
    "\n",
    "#acc = accuracy_score(y_true, threshold(y_pred))\n",
    "#print(\"accuracy =\" + str(acc))\n",
    "\n",
    "loss = log_loss(y_true, y_pred, eps = 1e-15, normalize = True, sample_weight = None, labels = None)\n",
    "print(\"cross entropy = \" + str(loss)) # the mean loss per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtQAAAHwCAYAAACG+PhNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIiATShFhEREREJ8L/k9/vme4CNVQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19fd9b5d780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "xgb.plot_importance(bst, height=0.5, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### mean normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy = 0.52011051747\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(df_X_train_mean_norm, label = y_train)\n",
    "dtest = xgb.DMatrix(df_X_test_mean_norm)\n",
    "\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "\n",
    "# make prediction\n",
    "y_pred = bst.predict(dtest)\n",
    "#print(y_pred)\n",
    "\n",
    "#acc = accuracy_score(y_true, y_pred = threshold(y_pred))\n",
    "#print(\"accuracy =\" + str(acc))\n",
    "loss = log_loss(y_true, y_pred, eps = 1e-15, normalize = True, sample_weight = None, labels = None)\n",
    "print(\"cross entropy = \" + str(loss)) # the mean loss per sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### minmax normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy = 0.530342265475\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(df_X_train_minmax_norm, label = y_train)\n",
    "dtest = xgb.DMatrix(df_X_test_minmax_norm)\n",
    "\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "\n",
    "# make prediction\n",
    "y_pred = bst.predict(dtest)\n",
    "\n",
    "loss = log_loss(y_true, y_pred, eps = 1e-15, normalize = True, sample_weight = None, labels = None)\n",
    "print(\"cross entropy = \" + str(loss)) # the mean loss per sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the cross entropy is higher when we normalize, we will process model with unormalized data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop to del some features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On essaye de supprimer les features un par un"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************** suppression de la 0e colonne     ********************\n",
      "cross entropy = 0.333443162362\n",
      "**************** suppression de la 1e colonne     ********************\n",
      "cross entropy = 0.328992929073\n",
      "**************** suppression de la 2e colonne     ********************\n",
      "cross entropy = 0.327547914759\n",
      "**************** suppression de la 3e colonne     ********************\n",
      "cross entropy = 0.327195596446\n",
      "**************** suppression de la 4e colonne     ********************\n",
      "cross entropy = 0.327802037901\n",
      "**************** suppression de la 5e colonne     ********************\n",
      "cross entropy = 0.327499173861\n",
      "**************** suppression de la 6e colonne     ********************\n",
      "cross entropy = 0.32708745982\n",
      "**************** suppression de la 7e colonne     ********************\n",
      "cross entropy = 0.327648883429\n",
      "**************** suppression de la 8e colonne     ********************\n",
      "cross entropy = 0.326902083537\n",
      "**************** suppression de la 9e colonne     ********************\n",
      "cross entropy = 0.325556082097\n",
      "**************** suppression de la 10e colonne     ********************\n",
      "cross entropy = 0.327534890567\n",
      "**************** suppression de la 11e colonne     ********************\n",
      "cross entropy = 0.327972822983\n",
      "**************** suppression de la 12e colonne     ********************\n",
      "cross entropy = 0.327641009369\n",
      "**************** suppression de la 13e colonne     ********************\n",
      "cross entropy = 0.326890231212\n",
      "**************** suppression de la 14e colonne     ********************\n",
      "cross entropy = 0.327705759973\n",
      "**************** suppression de la 15e colonne     ********************\n",
      "cross entropy = 0.327736478608\n",
      "**************** suppression de la 16e colonne     ********************\n",
      "cross entropy = 0.328503588264\n",
      "**************** suppression de la 17e colonne     ********************\n",
      "cross entropy = 0.327036831494\n",
      "**************** suppression de la 18e colonne     ********************\n",
      "cross entropy = 0.327176325761\n",
      "**************** suppression de la 19e colonne     ********************\n",
      "cross entropy = 0.327013428452\n",
      "**************** suppression de la 20e colonne     ********************\n",
      "cross entropy = 0.326687964846\n",
      "**************** suppression de la 21e colonne     ********************\n",
      "cross entropy = 0.327251944647\n",
      "**************** suppression de la 22e colonne     ********************\n",
      "cross entropy = 0.327080157831\n",
      "**************** suppression de la 23e colonne     ********************\n",
      "cross entropy = 0.327357357204\n",
      "**************** suppression de la 24e colonne     ********************\n",
      "cross entropy = 0.327973510795\n",
      "**************** suppression de la 25e colonne     ********************\n",
      "cross entropy = 0.327551775412\n",
      "**************** suppression de la 26e colonne     ********************\n",
      "cross entropy = 0.326662693826\n",
      "**************** suppression de la 27e colonne     ********************\n",
      "cross entropy = 0.326710650575\n",
      "**************** suppression de la 28e colonne     ********************\n",
      "cross entropy = 0.327295326063\n",
      "**************** suppression de la 29e colonne     ********************\n",
      "cross entropy = 0.327209419548\n",
      "**************** suppression de la 30e colonne     ********************\n",
      "cross entropy = 0.326719515219\n",
      "**************** suppression de la 31e colonne     ********************\n",
      "cross entropy = 0.327935195716\n",
      "**************** suppression de la 32e colonne     ********************\n",
      "cross entropy = 0.326763161611\n",
      "**************** suppression de la 33e colonne     ********************\n",
      "cross entropy = 0.32740086307\n",
      "**************** suppression de la 34e colonne     ********************\n",
      "cross entropy = 0.327389969139\n",
      "**************** suppression de la 35e colonne     ********************\n",
      "cross entropy = 0.328589097927\n",
      "**************** suppression de la 36e colonne     ********************\n",
      "cross entropy = 0.328074750113\n",
      "**************** suppression de la 37e colonne     ********************\n",
      "cross entropy = 0.325847511244\n",
      "**************** suppression de la 38e colonne     ********************\n",
      "cross entropy = 0.328098465402\n",
      "**************** suppression de la 39e colonne     ********************\n",
      "cross entropy = 0.327966352229\n",
      "**************** suppression de la 40e colonne     ********************\n",
      "cross entropy = 0.327219723773\n",
      "**************** suppression de la 41e colonne     ********************\n",
      "cross entropy = 0.327208600835\n",
      "**************** suppression de la 42e colonne     ********************\n",
      "cross entropy = 0.330685936159\n",
      "**************** suppression de la 43e colonne     ********************\n",
      "cross entropy = 0.32702802205\n",
      "**************** suppression de la 44e colonne     ********************\n",
      "cross entropy = 0.432645707358\n",
      "**************** suppression de la 45e colonne     ********************\n",
      "cross entropy = 0.386903192643\n",
      "**************** suppression de la 46e colonne     ********************\n",
      "cross entropy = 0.326969029427\n",
      "**************** suppression de la 47e colonne     ********************\n",
      "cross entropy = 0.327566378872\n",
      "**************** suppression de la 48e colonne     ********************\n",
      "cross entropy = 0.327613850418\n",
      "**************** suppression de la 49e colonne     ********************\n",
      "cross entropy = 0.326185910799\n",
      "**************** suppression de la 50e colonne     ********************\n",
      "cross entropy = 0.32717559959\n",
      "**************** suppression de la 51e colonne     ********************\n",
      "cross entropy = 0.327753726275\n",
      "**************** suppression de la 52e colonne     ********************\n",
      "cross entropy = 0.328013444565\n",
      "**************** suppression de la 53e colonne     ********************\n",
      "cross entropy = 0.327402326971\n",
      "**************** suppression de la 54e colonne     ********************\n",
      "cross entropy = 0.326777173511\n",
      "**************** suppression de la 55e colonne     ********************\n",
      "cross entropy = 0.32746935011\n",
      "**************** suppression de la 56e colonne     ********************\n",
      "cross entropy = 0.327340921084\n",
      "**************** suppression de la 57e colonne     ********************\n",
      "cross entropy = 0.326368434774\n",
      "**************** suppression de la 58e colonne     ********************\n",
      "cross entropy = 0.327558337891\n",
      "**************** suppression de la 59e colonne     ********************\n",
      "cross entropy = 0.327475220862\n",
      "**************** suppression de la 60e colonne     ********************\n",
      "cross entropy = 0.326753759054\n"
     ]
    }
   ],
   "source": [
    "loss_array_1=list()\n",
    "for i in range(len(features)):\n",
    "    g = X_train.copy()\n",
    "    h = X_test.copy()\n",
    "    print(\"**************** suppression de la \" + str(i) +\"e colonne     ********************\")\n",
    "    index=list()\n",
    "    index.append(i)\n",
    "    g = del_features(g, index)\n",
    "    h = del_features(h, index)\n",
    "    dtrain = xgb.DMatrix(g, label = y_train)\n",
    "    dtest = xgb.DMatrix(h)\n",
    "    bst = xgb.train(param, dtrain, num_round)\n",
    "    y_pred = bst.predict(dtest)\n",
    "    loss = log_loss(y_true, y_pred, eps = 1e-15, normalize = True, sample_weight = None, labels = None)\n",
    "    loss_array_1.append(loss)\n",
    "    print(\"cross entropy = \" + str(loss)) # the mean loss per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 37, 49, 57, 26]\n",
      "[0.32555608209723047, 0.32584751124443917, 0.32618591079873954, 0.32636843477363148, 0.32666269382624802]\n"
     ]
    }
   ],
   "source": [
    "# on garde les 5 plus basse cross entropy\n",
    "cross = loss_array_1.copy()\n",
    "sorted_cross = sorted(range(len(cross)), key=lambda k: cross[k])[:5]\n",
    "print(sorted_cross)\n",
    "print([cross[i] for i in sorted_cross])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va choisir le modèle qui minimise la cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On en supprime 2 parmi ceux qui donnent la moins bonne précision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************** suppresion des 9 - 37 colonnes     ********************\n",
      "cross entropy = 0.327489816505\n",
      "**************** suppresion des 9 - 49 colonnes     ********************\n",
      "cross entropy = 0.327429050539\n",
      "**************** suppresion des 9 - 57 colonnes     ********************\n",
      "cross entropy = 0.326972657719\n",
      "**************** suppresion des 9 - 26 colonnes     ********************\n",
      "cross entropy = 0.327272646434\n",
      "**************** suppresion des 37 - 49 colonnes     ********************\n",
      "cross entropy = 0.326926407531\n",
      "**************** suppresion des 37 - 57 colonnes     ********************\n",
      "cross entropy = 0.326735227502\n",
      "**************** suppresion des 37 - 26 colonnes     ********************\n",
      "cross entropy = 0.326360666679\n",
      "**************** suppresion des 49 - 57 colonnes     ********************\n",
      "cross entropy = 0.326628015853\n",
      "**************** suppresion des 49 - 26 colonnes     ********************\n",
      "cross entropy = 0.32686513676\n",
      "**************** suppresion des 57 - 26 colonnes     ********************\n",
      "cross entropy = 0.327330624332\n"
     ]
    }
   ],
   "source": [
    "loss_array_2=list()\n",
    "for i in range(len(sorted_cross)):\n",
    "    for j in range(i+1, len(sorted_cross), 1):\n",
    "        g = X_train.copy()\n",
    "        h = X_test.copy()\n",
    "        print(\"**************** suppresion des \" + str(sorted_cross[i])+\" - \"+ str(sorted_cross[j]) +\" colonnes     ********************\")\n",
    "        index=list()\n",
    "        index.append(sorted_cross[i])\n",
    "        index.append(sorted_cross[j])\n",
    "        g = del_features(g, index)\n",
    "        h = del_features(h, index)\n",
    "        dtrain = xgb.DMatrix(g, label = y_train)\n",
    "        dtest = xgb.DMatrix(h)\n",
    "        bst = xgb.train(param, dtrain, num_round)\n",
    "        y_pred = bst.predict(dtest)\n",
    "        loss = log_loss(y_true, y_pred, eps = 1e-15, normalize = True, sample_weight = None, labels = None)\n",
    "        print(\"cross entropy = \" + str(loss)) # the mean loss per sample.\n",
    "        loss_array_2.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 7, 5, 8, 4, 2, 3, 9, 1, 0]\n",
      "[0.32636066667880392, 0.32662801585288609, 0.32673522750249778, 0.32686513675999307, 0.32692640753087016, 0.32697265771913719, 0.32727264643438991, 0.32733062433196314, 0.32742905053877169, 0.32748981650476322]\n"
     ]
    }
   ],
   "source": [
    "cross2 = loss_array_2.copy()\n",
    "sorted_cross_2 = sorted(range(len(cross2)), key=lambda k: cross2[k])\n",
    "print(sorted_cross_2)\n",
    "print([cross2[i] for i in sorted_cross_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy maximum quand on supprime les colonnes 11 et 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Si on en supprime plusieurs (à faire manuellement trop long pour boucler dessus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56070, 46)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy = 0.327861498543\n"
     ]
    }
   ],
   "source": [
    "g = X_train.copy()\n",
    "h = X_test.copy() \n",
    "index = [57, 49, 9] # a mettre en ordre déroissant\n",
    "g = del_features(g, index)\n",
    "h = del_features(h, index)\n",
    "dtrain = xgb.DMatrix(g, label = y_train)\n",
    "dtest = xgb.DMatrix(h)\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "y_pred = bst.predict(dtest)\n",
    "loss = log_loss(y_true, y_pred, eps = 1e-15, normalize = True, sample_weight = None, labels = None)\n",
    "print(\"cross entropy = \" + str(loss)) # the mean loss per sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model final\n",
      "cross entropy = 0.353712724939\n"
     ]
    }
   ],
   "source": [
    "param = {'max_depth': 8, 'eta':0.2, 'silent':1, \"objective\" : \"binary:logistic\", \"eval_metric\": \"logloss\"} #'metric': 'binary_logloss',\n",
    "num_round = 320\n",
    "g = X_train.copy()\n",
    "h = X_test.copy()\n",
    "index = [9]\n",
    "g = del_features(g, index)\n",
    "h = del_features(h, index)\n",
    "dtrain = xgb.DMatrix(g, label = y_train)\n",
    "dtest = xgb.DMatrix(h)\n",
    "\n",
    "# This is used to determine the best round to stop on.\n",
    "#print('First model ')\n",
    "#bst = xgb.train(param, dtrain, 1000) \n",
    "#print(bst.best_iteration)\n",
    "#num_round = bst.best_iteration\n",
    "\n",
    "print('Model final')\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "y_pred = bst.predict(dtest)\n",
    "\n",
    "#acc = accuracy_score(y_true, threshold(y_pred))\n",
    "#print(\"accuracy = \" + str(acc))\n",
    "\n",
    "loss = log_loss(y_true, y_pred, eps = 1e-15, normalize = True, sample_weight = None, labels = None)\n",
    "print(\"cross entropy = \" + str(loss)) # the mean loss per sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecrire les données test dans un csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attention: \n",
    "Soit on prédict direct sur nos vrai données test à partir du modèle au dessus (le meilleur)\n",
    "\n",
    "soit on réentraine sur toutes nos données labellisées le meilleur model et on prédit sur les données test.\n",
    "(dans ce cas on a 20000 données d'entrainement en plus').\n",
    "\n",
    "On peut générer un csv pour les deux cas et voir le mieux au leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20179, 5)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv', names=['row_ID', 'text_a_ID', 'text_b_ID', 'text_a_text', 'text_b_text', 'target'])\n",
    "#train, test = train_test_split(train, test_size = 0.3)\n",
    "real_test = pd.read_csv('test.csv', names=['row_ID', 'text_a_ID', 'text_b_ID', 'text_a_text', 'text_b_text'])\n",
    "real_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real_test['target'] = 7 # nb different de 0 et 1 mais défini pour réutiliser les mm fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_ID</th>\n",
       "      <th>text_a_ID</th>\n",
       "      <th>text_b_ID</th>\n",
       "      <th>text_a_text</th>\n",
       "      <th>text_b_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>199954</td>\n",
       "      <td>384085</td>\n",
       "      <td>What are the some of the best novels?</td>\n",
       "      <td>What are some of the greatest novels of all ti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>128681</td>\n",
       "      <td>237407</td>\n",
       "      <td>What are the pictures that made you look twice?</td>\n",
       "      <td>What are some amazing pictures one has to see ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>170846</td>\n",
       "      <td>240621</td>\n",
       "      <td>Have the ellectoral college members ever voted...</td>\n",
       "      <td>When has the electoral college voted against t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>55110</td>\n",
       "      <td>177468</td>\n",
       "      <td>Did Ravana really have 10 heads?</td>\n",
       "      <td>Why did Ravana have 10 heads?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>425513</td>\n",
       "      <td>400256</td>\n",
       "      <td>What's a book that you feel helped you to impr...</td>\n",
       "      <td>What books or magazines should I read to impro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>105990</td>\n",
       "      <td>256943</td>\n",
       "      <td>Is astrology true? Should we believe it or not?</td>\n",
       "      <td>Should you believe in astrology and astrologer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>366314</td>\n",
       "      <td>224793</td>\n",
       "      <td>What are some of the biggest lies that you eve...</td>\n",
       "      <td>What is the biggest lie ever told by any gover...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>167249</td>\n",
       "      <td>167582</td>\n",
       "      <td>How can I advertise my YouTube Channel to get ...</td>\n",
       "      <td>How can I get more traffic to my YouTube videos?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>344939</td>\n",
       "      <td>281696</td>\n",
       "      <td>Is time travel possible through cosmic strings?</td>\n",
       "      <td>Is time travel already possible on Earth?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>107721</td>\n",
       "      <td>138968</td>\n",
       "      <td>What should I do to earn money online?</td>\n",
       "      <td>How can I earn money on internet?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>70380</td>\n",
       "      <td>189775</td>\n",
       "      <td>Why did you lose your virginity?</td>\n",
       "      <td>What is the best way to lose your virginity?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>422283</td>\n",
       "      <td>387926</td>\n",
       "      <td>What does surgical strike mean?</td>\n",
       "      <td>What is the surgical strike?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>353154</td>\n",
       "      <td>165955</td>\n",
       "      <td>Is it safe to apply aloe vera gel on the face ...</td>\n",
       "      <td>How can I buy the best aloe vera gel for face?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>116890</td>\n",
       "      <td>223712</td>\n",
       "      <td>Does drinking lemon juice in the morning on an...</td>\n",
       "      <td>What are the best way to reduce belly fat and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>56193</td>\n",
       "      <td>284398</td>\n",
       "      <td>What is the hardest thing(s) about raising chi...</td>\n",
       "      <td>What is the hardest thing(s) about raising chi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>192860</td>\n",
       "      <td>103497</td>\n",
       "      <td>What is the best way to spank someone?</td>\n",
       "      <td>What is the best way to spank myself?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>419061</td>\n",
       "      <td>401379</td>\n",
       "      <td>How can I tell if someone blocked me on snapchat?</td>\n",
       "      <td>Someone blocked me on snapchat. How can I dest...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>365562</td>\n",
       "      <td>261605</td>\n",
       "      <td>What makes a person truly happy?</td>\n",
       "      <td>What truly makes you happy?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>407441</td>\n",
       "      <td>232114</td>\n",
       "      <td>How does Hillary Clinton view US-India relatio...</td>\n",
       "      <td>What is the foreign policy of the United State...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>26559</td>\n",
       "      <td>148265</td>\n",
       "      <td>How do I start learning machine learning?</td>\n",
       "      <td>What is the best way to get started with Machi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>17731</td>\n",
       "      <td>345422</td>\n",
       "      <td>Who are some of the best underrated actors in ...</td>\n",
       "      <td>Who is the most versatile but underrated actre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>365785</td>\n",
       "      <td>246499</td>\n",
       "      <td>Are there any theories on what caused the big ...</td>\n",
       "      <td>What caused the Big Bang?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>253071</td>\n",
       "      <td>379651</td>\n",
       "      <td>Is is safe to take laxative pills to lose weig...</td>\n",
       "      <td>How do laxative teas make you lose weight?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>191369</td>\n",
       "      <td>110891</td>\n",
       "      <td>What is Hillary Clinton's diplomatic attitude ...</td>\n",
       "      <td>What will be Hillary Clinton's policy towards ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>415414</td>\n",
       "      <td>208199</td>\n",
       "      <td>What is one thing you want to do before you die?</td>\n",
       "      <td>What do you want to do before you die?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>284484</td>\n",
       "      <td>407521</td>\n",
       "      <td>What is the meaning of \"brisado\"?</td>\n",
       "      <td>What is the meaning of meaning?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>216343</td>\n",
       "      <td>420785</td>\n",
       "      <td>Why do people ask Quora questions which can be...</td>\n",
       "      <td>Why don't Quora people just look up the answer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>167736</td>\n",
       "      <td>124268</td>\n",
       "      <td>How I can I logout from Quora?</td>\n",
       "      <td>How should I logout from qoura?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>419030</td>\n",
       "      <td>355200</td>\n",
       "      <td>Why are all my questions immediately being mar...</td>\n",
       "      <td>Why do my dank questions keep being marked as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>48073</td>\n",
       "      <td>311559</td>\n",
       "      <td>What would be the next step by Narendra Modi t...</td>\n",
       "      <td>What is going to be Modi s next step against d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20149</th>\n",
       "      <td>20149</td>\n",
       "      <td>103125</td>\n",
       "      <td>409631</td>\n",
       "      <td>How can I get started using Quora?</td>\n",
       "      <td>How do I get started using Quora? Why all my w...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20150</th>\n",
       "      <td>20150</td>\n",
       "      <td>314384</td>\n",
       "      <td>65383</td>\n",
       "      <td>How can I convert audible AAX files to MP3 mac?</td>\n",
       "      <td>How can I convert audible AAX files to MP3?</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20151</th>\n",
       "      <td>20151</td>\n",
       "      <td>203865</td>\n",
       "      <td>223489</td>\n",
       "      <td>Does sure gell help pass a drug test for meth?</td>\n",
       "      <td>Can hair dye help pass a hair follicle drug te...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20152</th>\n",
       "      <td>20152</td>\n",
       "      <td>43855</td>\n",
       "      <td>340063</td>\n",
       "      <td>Does massage really increase breast size?</td>\n",
       "      <td>Do breast massages really help in increasing b...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20153</th>\n",
       "      <td>20153</td>\n",
       "      <td>353886</td>\n",
       "      <td>205442</td>\n",
       "      <td>How can one track a mobile number location?</td>\n",
       "      <td>How can I track a mobile number and the locati...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20154</th>\n",
       "      <td>20154</td>\n",
       "      <td>295838</td>\n",
       "      <td>286664</td>\n",
       "      <td>How did you quit/stop smoking?</td>\n",
       "      <td>How can I stop smoking?</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20155</th>\n",
       "      <td>20155</td>\n",
       "      <td>58482</td>\n",
       "      <td>127486</td>\n",
       "      <td>I want to live the rest of my life alone and w...</td>\n",
       "      <td>How do I accept that I will live alone the res...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20156</th>\n",
       "      <td>20156</td>\n",
       "      <td>451324</td>\n",
       "      <td>113225</td>\n",
       "      <td>How did Trump win America's vote?</td>\n",
       "      <td>How did Donald Trump win the election?</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20157</th>\n",
       "      <td>20157</td>\n",
       "      <td>46094</td>\n",
       "      <td>347874</td>\n",
       "      <td>What is the biggest mistake people do when the...</td>\n",
       "      <td>What are the biggest mistakes people make when...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20158</th>\n",
       "      <td>20158</td>\n",
       "      <td>367645</td>\n",
       "      <td>287563</td>\n",
       "      <td>What do most Americans (from the US) think of ...</td>\n",
       "      <td>What do American girls think about Indian guys?</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20159</th>\n",
       "      <td>20159</td>\n",
       "      <td>232757</td>\n",
       "      <td>358272</td>\n",
       "      <td>What is the craziest question ever asked on Qu...</td>\n",
       "      <td>What is the strangest question on Quora?</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20160</th>\n",
       "      <td>20160</td>\n",
       "      <td>274892</td>\n",
       "      <td>122148</td>\n",
       "      <td>How can I improve my realistic drawings?</td>\n",
       "      <td>How can I improve my drawing ?</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161</th>\n",
       "      <td>20161</td>\n",
       "      <td>411133</td>\n",
       "      <td>341720</td>\n",
       "      <td>What are some of the best Hollywood movies to ...</td>\n",
       "      <td>Which is the all time best Hollywood movie?</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20162</th>\n",
       "      <td>20162</td>\n",
       "      <td>118359</td>\n",
       "      <td>110608</td>\n",
       "      <td>Self-Improvement: How can I motivate myself to...</td>\n",
       "      <td>Why do I lack motivation to work hard?</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20163</th>\n",
       "      <td>20163</td>\n",
       "      <td>442704</td>\n",
       "      <td>440298</td>\n",
       "      <td>How do I delete a question from Quora?</td>\n",
       "      <td>Can I delete all the questions I asked on Quora?</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20164</th>\n",
       "      <td>20164</td>\n",
       "      <td>301151</td>\n",
       "      <td>354200</td>\n",
       "      <td>Why is Hillary Clinton a better choice than Do...</td>\n",
       "      <td>Is Hillary Clinton really worse than Donald Tr...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20165</th>\n",
       "      <td>20165</td>\n",
       "      <td>28097</td>\n",
       "      <td>377142</td>\n",
       "      <td>How does 1 Billion Rising stop violence agains...</td>\n",
       "      <td>When will Indians learn to respect women and s...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20166</th>\n",
       "      <td>20166</td>\n",
       "      <td>256441</td>\n",
       "      <td>265093</td>\n",
       "      <td>What is the scope after physics honours?</td>\n",
       "      <td>What is the scope after bsc physics?</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20167</th>\n",
       "      <td>20167</td>\n",
       "      <td>368041</td>\n",
       "      <td>13426</td>\n",
       "      <td>Why do so many people hate nickelback?</td>\n",
       "      <td>Why don't people like Nickelback?</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20168</th>\n",
       "      <td>20168</td>\n",
       "      <td>92752</td>\n",
       "      <td>310096</td>\n",
       "      <td>What skills are needed for machine learning jobs?</td>\n",
       "      <td>What other skills do you need as a programmer?</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20169</th>\n",
       "      <td>20169</td>\n",
       "      <td>115691</td>\n",
       "      <td>109133</td>\n",
       "      <td>How can you tell if a guy likes you or not?</td>\n",
       "      <td>How can you tell a if guy likes you?</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20170</th>\n",
       "      <td>20170</td>\n",
       "      <td>317191</td>\n",
       "      <td>392872</td>\n",
       "      <td>What is the best rap song ever made?</td>\n",
       "      <td>What are the best rap songs of all time?</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20171</th>\n",
       "      <td>20171</td>\n",
       "      <td>198686</td>\n",
       "      <td>61805</td>\n",
       "      <td>What is the best method of learning to speak a...</td>\n",
       "      <td>What is the best method to learn new language?</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20172</th>\n",
       "      <td>20172</td>\n",
       "      <td>264474</td>\n",
       "      <td>122621</td>\n",
       "      <td>Is PM Narendra Modi the brand ambassador of Re...</td>\n",
       "      <td>Why did Narendra Modi allow Reliance to publis...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20173</th>\n",
       "      <td>20173</td>\n",
       "      <td>344052</td>\n",
       "      <td>129175</td>\n",
       "      <td>How does one become a video game designer or d...</td>\n",
       "      <td>What should we do to become a game designer?</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20174</th>\n",
       "      <td>20174</td>\n",
       "      <td>407592</td>\n",
       "      <td>178407</td>\n",
       "      <td>Montana State Football Live Stream | Watch Mon...</td>\n",
       "      <td>Idaho State Football Live Stream | Watch Idaho...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20175</th>\n",
       "      <td>20175</td>\n",
       "      <td>394268</td>\n",
       "      <td>56248</td>\n",
       "      <td>Why nobody answer my questions in Quora?</td>\n",
       "      <td>Why do people never answer my question on Quora?</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20176</th>\n",
       "      <td>20176</td>\n",
       "      <td>146511</td>\n",
       "      <td>200961</td>\n",
       "      <td>What is it like to have a huge penis?</td>\n",
       "      <td>Whats it like to have a huge penis?</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20177</th>\n",
       "      <td>20177</td>\n",
       "      <td>37354</td>\n",
       "      <td>401247</td>\n",
       "      <td>How much does each miner in Gold Rush: Alaska ...</td>\n",
       "      <td>What is the biggest gold nugget found on a TV ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20178</th>\n",
       "      <td>20178</td>\n",
       "      <td>282027</td>\n",
       "      <td>425313</td>\n",
       "      <td>What are all the places that I can visit in Ch...</td>\n",
       "      <td>What are the places to visit in Chennai?</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100279 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       row_ID  text_a_ID  text_b_ID  \\\n",
       "0           0     199954     384085   \n",
       "1           1     128681     237407   \n",
       "2           2     170846     240621   \n",
       "3           3      55110     177468   \n",
       "4           4     425513     400256   \n",
       "5           5     105990     256943   \n",
       "6           6     366314     224793   \n",
       "7           7     167249     167582   \n",
       "8           8     344939     281696   \n",
       "9           9     107721     138968   \n",
       "10         10      70380     189775   \n",
       "11         11     422283     387926   \n",
       "12         12     353154     165955   \n",
       "13         13     116890     223712   \n",
       "14         14      56193     284398   \n",
       "15         15     192860     103497   \n",
       "16         16     419061     401379   \n",
       "17         17     365562     261605   \n",
       "18         18     407441     232114   \n",
       "19         19      26559     148265   \n",
       "20         20      17731     345422   \n",
       "21         21     365785     246499   \n",
       "22         22     253071     379651   \n",
       "23         23     191369     110891   \n",
       "24         24     415414     208199   \n",
       "25         25     284484     407521   \n",
       "26         26     216343     420785   \n",
       "27         27     167736     124268   \n",
       "28         28     419030     355200   \n",
       "29         29      48073     311559   \n",
       "...       ...        ...        ...   \n",
       "20149   20149     103125     409631   \n",
       "20150   20150     314384      65383   \n",
       "20151   20151     203865     223489   \n",
       "20152   20152      43855     340063   \n",
       "20153   20153     353886     205442   \n",
       "20154   20154     295838     286664   \n",
       "20155   20155      58482     127486   \n",
       "20156   20156     451324     113225   \n",
       "20157   20157      46094     347874   \n",
       "20158   20158     367645     287563   \n",
       "20159   20159     232757     358272   \n",
       "20160   20160     274892     122148   \n",
       "20161   20161     411133     341720   \n",
       "20162   20162     118359     110608   \n",
       "20163   20163     442704     440298   \n",
       "20164   20164     301151     354200   \n",
       "20165   20165      28097     377142   \n",
       "20166   20166     256441     265093   \n",
       "20167   20167     368041      13426   \n",
       "20168   20168      92752     310096   \n",
       "20169   20169     115691     109133   \n",
       "20170   20170     317191     392872   \n",
       "20171   20171     198686      61805   \n",
       "20172   20172     264474     122621   \n",
       "20173   20173     344052     129175   \n",
       "20174   20174     407592     178407   \n",
       "20175   20175     394268      56248   \n",
       "20176   20176     146511     200961   \n",
       "20177   20177      37354     401247   \n",
       "20178   20178     282027     425313   \n",
       "\n",
       "                                             text_a_text  \\\n",
       "0                  What are the some of the best novels?   \n",
       "1        What are the pictures that made you look twice?   \n",
       "2      Have the ellectoral college members ever voted...   \n",
       "3                       Did Ravana really have 10 heads?   \n",
       "4      What's a book that you feel helped you to impr...   \n",
       "5        Is astrology true? Should we believe it or not?   \n",
       "6      What are some of the biggest lies that you eve...   \n",
       "7      How can I advertise my YouTube Channel to get ...   \n",
       "8        Is time travel possible through cosmic strings?   \n",
       "9                 What should I do to earn money online?   \n",
       "10                      Why did you lose your virginity?   \n",
       "11                       What does surgical strike mean?   \n",
       "12     Is it safe to apply aloe vera gel on the face ...   \n",
       "13     Does drinking lemon juice in the morning on an...   \n",
       "14     What is the hardest thing(s) about raising chi...   \n",
       "15                What is the best way to spank someone?   \n",
       "16     How can I tell if someone blocked me on snapchat?   \n",
       "17                      What makes a person truly happy?   \n",
       "18     How does Hillary Clinton view US-India relatio...   \n",
       "19             How do I start learning machine learning?   \n",
       "20     Who are some of the best underrated actors in ...   \n",
       "21     Are there any theories on what caused the big ...   \n",
       "22     Is is safe to take laxative pills to lose weig...   \n",
       "23     What is Hillary Clinton's diplomatic attitude ...   \n",
       "24      What is one thing you want to do before you die?   \n",
       "25                     What is the meaning of \"brisado\"?   \n",
       "26     Why do people ask Quora questions which can be...   \n",
       "27                        How I can I logout from Quora?   \n",
       "28     Why are all my questions immediately being mar...   \n",
       "29     What would be the next step by Narendra Modi t...   \n",
       "...                                                  ...   \n",
       "20149                 How can I get started using Quora?   \n",
       "20150    How can I convert audible AAX files to MP3 mac?   \n",
       "20151     Does sure gell help pass a drug test for meth?   \n",
       "20152          Does massage really increase breast size?   \n",
       "20153        How can one track a mobile number location?   \n",
       "20154                     How did you quit/stop smoking?   \n",
       "20155  I want to live the rest of my life alone and w...   \n",
       "20156                  How did Trump win America's vote?   \n",
       "20157  What is the biggest mistake people do when the...   \n",
       "20158  What do most Americans (from the US) think of ...   \n",
       "20159  What is the craziest question ever asked on Qu...   \n",
       "20160           How can I improve my realistic drawings?   \n",
       "20161  What are some of the best Hollywood movies to ...   \n",
       "20162  Self-Improvement: How can I motivate myself to...   \n",
       "20163             How do I delete a question from Quora?   \n",
       "20164  Why is Hillary Clinton a better choice than Do...   \n",
       "20165  How does 1 Billion Rising stop violence agains...   \n",
       "20166           What is the scope after physics honours?   \n",
       "20167             Why do so many people hate nickelback?   \n",
       "20168  What skills are needed for machine learning jobs?   \n",
       "20169        How can you tell if a guy likes you or not?   \n",
       "20170               What is the best rap song ever made?   \n",
       "20171  What is the best method of learning to speak a...   \n",
       "20172  Is PM Narendra Modi the brand ambassador of Re...   \n",
       "20173  How does one become a video game designer or d...   \n",
       "20174  Montana State Football Live Stream | Watch Mon...   \n",
       "20175           Why nobody answer my questions in Quora?   \n",
       "20176              What is it like to have a huge penis?   \n",
       "20177  How much does each miner in Gold Rush: Alaska ...   \n",
       "20178  What are all the places that I can visit in Ch...   \n",
       "\n",
       "                                             text_b_text  target  \n",
       "0      What are some of the greatest novels of all ti...       0  \n",
       "1      What are some amazing pictures one has to see ...       0  \n",
       "2      When has the electoral college voted against t...       1  \n",
       "3                          Why did Ravana have 10 heads?       1  \n",
       "4      What books or magazines should I read to impro...       0  \n",
       "5      Should you believe in astrology and astrologer...       1  \n",
       "6      What is the biggest lie ever told by any gover...       0  \n",
       "7       How can I get more traffic to my YouTube videos?       0  \n",
       "8              Is time travel already possible on Earth?       1  \n",
       "9                      How can I earn money on internet?       1  \n",
       "10          What is the best way to lose your virginity?       0  \n",
       "11                          What is the surgical strike?       1  \n",
       "12        How can I buy the best aloe vera gel for face?       0  \n",
       "13     What are the best way to reduce belly fat and ...       0  \n",
       "14     What is the hardest thing(s) about raising chi...       0  \n",
       "15                 What is the best way to spank myself?       0  \n",
       "16     Someone blocked me on snapchat. How can I dest...       0  \n",
       "17                           What truly makes you happy?       1  \n",
       "18     What is the foreign policy of the United State...       1  \n",
       "19     What is the best way to get started with Machi...       1  \n",
       "20     Who is the most versatile but underrated actre...       1  \n",
       "21                             What caused the Big Bang?       1  \n",
       "22            How do laxative teas make you lose weight?       1  \n",
       "23     What will be Hillary Clinton's policy towards ...       1  \n",
       "24                What do you want to do before you die?       1  \n",
       "25                       What is the meaning of meaning?       0  \n",
       "26     Why don't Quora people just look up the answer...       1  \n",
       "27                       How should I logout from qoura?       1  \n",
       "28     Why do my dank questions keep being marked as ...       1  \n",
       "29     What is going to be Modi s next step against d...       1  \n",
       "...                                                  ...     ...  \n",
       "20149  How do I get started using Quora? Why all my w...       7  \n",
       "20150        How can I convert audible AAX files to MP3?       7  \n",
       "20151  Can hair dye help pass a hair follicle drug te...       7  \n",
       "20152  Do breast massages really help in increasing b...       7  \n",
       "20153  How can I track a mobile number and the locati...       7  \n",
       "20154                            How can I stop smoking?       7  \n",
       "20155  How do I accept that I will live alone the res...       7  \n",
       "20156             How did Donald Trump win the election?       7  \n",
       "20157  What are the biggest mistakes people make when...       7  \n",
       "20158    What do American girls think about Indian guys?       7  \n",
       "20159           What is the strangest question on Quora?       7  \n",
       "20160                     How can I improve my drawing ?       7  \n",
       "20161        Which is the all time best Hollywood movie?       7  \n",
       "20162             Why do I lack motivation to work hard?       7  \n",
       "20163   Can I delete all the questions I asked on Quora?       7  \n",
       "20164  Is Hillary Clinton really worse than Donald Tr...       7  \n",
       "20165  When will Indians learn to respect women and s...       7  \n",
       "20166               What is the scope after bsc physics?       7  \n",
       "20167                  Why don't people like Nickelback?       7  \n",
       "20168     What other skills do you need as a programmer?       7  \n",
       "20169               How can you tell a if guy likes you?       7  \n",
       "20170           What are the best rap songs of all time?       7  \n",
       "20171     What is the best method to learn new language?       7  \n",
       "20172  Why did Narendra Modi allow Reliance to publis...       7  \n",
       "20173       What should we do to become a game designer?       7  \n",
       "20174  Idaho State Football Live Stream | Watch Idaho...       7  \n",
       "20175   Why do people never answer my question on Quora?       7  \n",
       "20176                Whats it like to have a huge penis?       7  \n",
       "20177  What is the biggest gold nugget found on a TV ...       7  \n",
       "20178           What are the places to visit in Chennai?       7  \n",
       "\n",
       "[100279 rows x 6 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_all_real_texts = pd.concat([train, test, real_test])\n",
    "df_all_texts = pd.concat([train, real_test])\n",
    "df_all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda_init(pairs_train, pairs_test, df_all_texts):\n",
    "    NUM_TOPICS = 300\n",
    "    RANDOM_SEED = 42\n",
    "    \n",
    "    lda_tokens = list()\n",
    "    for i in range(len(pairs_train)):\n",
    "        lda_tokens.append(stem_pair(pairs_train[i]))\n",
    "    for i in range(len(pairs_test)):\n",
    "        lda_tokens.append(stem_pair(pairs_test[i]))\n",
    "    lda_documents = list(np.array(lda_tokens).ravel()) #When a view is desired, arr.reshape(-1) may be preferable.\n",
    "    lda_dictionary = Dictionary(lda_documents)\n",
    "    lda_corpus = [lda_dictionary.doc2bow(document) for document in lda_documents]\n",
    "\n",
    "    model = LdaMulticore(\n",
    "        lda_corpus,\n",
    "        num_topics=NUM_TOPICS,\n",
    "        id2word=lda_dictionary,\n",
    "        random_state=RANDOM_SEED,\n",
    "    )\n",
    "\n",
    "    lda_distances = list()\n",
    "    for i in lda_tokens:\n",
    "        lda_distances.append(compute_topic_distances(i[0], i[1],lda_dictionary, model))\n",
    "\n",
    "    lda_train = np.array(lda_distances[:len(pairs_train)], dtype='float64')\n",
    "    lda_test = np.array(lda_distances[len(pairs_train):], dtype='float64')\n",
    "    \n",
    "    columns_lda=['lda_1','lda_2']\n",
    "    lda_distances = pd.DataFrame(\n",
    "    lda_distances,\n",
    "    columns=columns_lda\n",
    "    )\n",
    "    return lda_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_ner_tags(df_all_texts):\n",
    "    pos_tags_whitelist = ['ADJ', 'ADV', 'NOUN', 'PROPN', 'NUM', 'VERB']\n",
    "    ner_tags_whitelist = ['GPE', 'LOC', 'ORG', 'NORP', 'PERSON', 'PRODUCT', 'DATE', 'TIME', 'QUANTITY', 'CARDINAL']\n",
    "\n",
    "    num_raw_features = len(pos_tags_whitelist) + len(ner_tags_whitelist)\n",
    "\n",
    "    X1 = create_counter('text_a_text', df_all_texts, num_raw_features, pos_tags_whitelist, ner_tags_whitelist)\n",
    "    X2 = create_counter('text_b_text', df_all_texts, num_raw_features, pos_tags_whitelist, ner_tags_whitelist)\n",
    "\n",
    "    df_pos_q1 = pd.DataFrame(\n",
    "        X1[:, 0:len(pos_tags_whitelist)],\n",
    "        columns=['pos_q1_' + pos_tag.lower() for pos_tag in pos_tags_whitelist])\n",
    "    df_pos_q2 = pd.DataFrame(\n",
    "        X2[:, 0:len(pos_tags_whitelist)],\n",
    "        columns=['pos_q2_' + pos_tag.lower() for pos_tag in pos_tags_whitelist])\n",
    "    df_ner_q1 = pd.DataFrame(\n",
    "        X1[:, -len(ner_tags_whitelist):],\n",
    "        columns=['ner_q1_' + ner_tag.lower() for ner_tag in ner_tags_whitelist])\n",
    "    df_ner_q2 = pd.DataFrame(\n",
    "        X2[:, -len(ner_tags_whitelist):],\n",
    "        columns=['ner_q2_' + ner_tag.lower() for ner_tag in ner_tags_whitelist])\n",
    "    \n",
    "    \n",
    "    tags_distances = list()\n",
    "    for i in list(range(len(df_all_texts))):\n",
    "        tags_distances.append(get_vector_distances(i,X1, X2, pos_tags_whitelist, ner_tags_whitelist))\n",
    "\n",
    "    tags_columns=[\n",
    "            'pos_tag_cosine',\n",
    "            'pos_tag_euclidean',\n",
    "            'ner_tag_euclidean',\n",
    "            'ner_tag_count_diff',\n",
    "        ]\n",
    "\n",
    "    tags_distances = pd.DataFrame(tags_distances, columns = tags_columns)\n",
    "    #df_tags = pd.concat([tags_distances, df_all_texts['text_a_ID'], df_all_texts['text_b_ID'] ], axis=1)\n",
    "    #return df_tags\n",
    "    return tags_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "lda\n",
      "tags\n",
      "freq\n",
      "Features construction\n",
      "1\n",
      "1001\n",
      "2001\n",
      "3001\n",
      "4001\n",
      "5001\n",
      "6001\n",
      "7001\n",
      "8001\n",
      "9001\n",
      "10001\n",
      "11001\n",
      "12001\n",
      "13001\n",
      "14001\n",
      "15001\n",
      "16001\n",
      "17001\n",
      "18001\n",
      "19001\n",
      "20001\n",
      "21001\n",
      "22001\n",
      "23001\n",
      "24001\n",
      "25001\n",
      "26001\n",
      "27001\n",
      "28001\n",
      "29001\n",
      "30001\n",
      "31001\n",
      "32001\n",
      "33001\n",
      "34001\n",
      "35001\n",
      "36001\n",
      "37001\n",
      "38001\n",
      "39001\n",
      "40001\n",
      "41001\n",
      "42001\n",
      "43001\n",
      "44001\n",
      "45001\n",
      "46001\n",
      "47001\n",
      "48001\n",
      "49001\n",
      "50001\n",
      "51001\n",
      "52001\n",
      "53001\n",
      "54001\n",
      "55001\n",
      "56001\n",
      "57001\n",
      "58001\n",
      "59001\n",
      "60001\n",
      "61001\n",
      "62001\n",
      "63001\n",
      "64001\n",
      "65001\n",
      "66001\n",
      "67001\n",
      "68001\n",
      "69001\n",
      "70001\n",
      "71001\n",
      "72001\n",
      "73001\n",
      "74001\n",
      "75001\n",
      "76001\n",
      "77001\n",
      "78001\n",
      "79001\n",
      "80001\n",
      "1\n",
      "1001\n",
      "2001\n",
      "3001\n",
      "4001\n",
      "5001\n",
      "6001\n",
      "7001\n",
      "8001\n",
      "9001\n",
      "10001\n",
      "11001\n",
      "12001\n",
      "13001\n",
      "14001\n",
      "15001\n",
      "16001\n",
      "17001\n",
      "18001\n",
      "19001\n",
      "20001\n"
     ]
    }
   ],
   "source": [
    "#print('Pairs construction')\n",
    "texts, pairs_train, pairs_test, y_train, y_true, ids2ind = construct_pairs(train, \n",
    "                                                                          real_test, \n",
    "                                                                          remove_stopwords = True, \n",
    "                                                                          pos_filtering = False, \n",
    "                                                                          stemming = True)\n",
    "print('A')\n",
    "A = tfIdf(texts)\n",
    "\n",
    "print('lda')\n",
    "\n",
    "#lda_distances = lda_init(pairs_train, pairs_test, df_all_texts)\n",
    "#lda_df=pd.DataFrame([lda_distances.lda_1.values,lda_distances.lda_2.values, df_all_texts['text_a_ID'].values, df_all_texts['text_b_ID'].values]).T \n",
    "#lda_df.columns=['lda_1', 'lda_2', 'text_a_ID','text_b_ID']\n",
    "\n",
    "#save_lda_df(lda_df, \"lda_df_real.csv\")\n",
    "lda_df = pd.read_csv('lda_df_real.csv')\n",
    "\n",
    "print('tags')\n",
    "#tags_distances = pos_ner_tags(df_all_texts)\n",
    "#df_tags=pd.DataFrame([tags_distances.pos_tag_cosine.values,\n",
    "#                tags_distances.pos_tag_euclidean.values, \n",
    "#                tags_distances.ner_tag_euclidean.values,\n",
    "#                tags_distances.ner_tag_count_diff.values,\n",
    "#                df_all_texts['text_a_ID'].values,\n",
    "#                df_all_texts['text_b_ID'].values]).T \n",
    "#df_tags.columns=['pos_tag_cosine', 'pos_tag_euclidean','ner_tag_euclidean','ner_tag_count_diff', 'text_a_ID','text_b_ID']\n",
    "#df_tags\n",
    "#save_df_tags(df_tags,\"df_tags_real.csv\")\n",
    "df_tags = pd.read_csv('df_tags_real.csv')\n",
    "\n",
    "print('freq')\n",
    "train_freq, test_freq = compute_question_freq(train, real_test)\n",
    "\n",
    "print('Features construction')\n",
    "X_train = construct_data(pairs_train, A, lda_df, df_tags, train_freq)\n",
    "X_test = construct_data(pairs_test, A, lda_df, df_tags, test_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "pd.DataFrame(X_train).to_csv('X_train_processed_real.csv', index=False, header=False)\n",
    "pd.DataFrame(X_test).to_csv('X_test_processed_real.csv', index=False, header=False)\n",
    "pd.DataFrame(y_train).to_csv('y_train_processed_real.csv', index=False, header=False)\n",
    "pd.DataFrame(y_true).to_csv('y_test_processed_real.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XX_train=X_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XX_train=pd.DataFrame(XX_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XX_train['target']=y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80100"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(XX_train, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yy_train = train['target']\n",
    "yy_test = test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train['target']\n",
    "del test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = train.copy()\n",
    "h = test.copy()\n",
    "\n",
    "dtrain = xgb.DMatrix(g, label = yy_train)\n",
    "dtest = xgb.DMatrix(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model final\n",
      "cross entropy = 0.307629078216\n"
     ]
    }
   ],
   "source": [
    "param = {'max_depth': 6, 'eta':0.1, 'silent':1, \"objective\" : \"binary:logistic\", \"eval_metric\": \"logloss\"} #'metric': 'binary_logloss',\n",
    "num_round = 400\n",
    "\n",
    "# This is used to determine the best round to stop on.\n",
    "#print('First model ')\n",
    "#bst = xgb.train(param, dtrain, 1000) \n",
    "#print(bst.best_iteration)\n",
    "#num_round = bst.best_iteration\n",
    "\n",
    "print('Model final')\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "yy_pred = bst.predict(dtest)\n",
    "\n",
    "loss = log_loss(yy_test, yy_pred, eps = 1e-15, normalize = True, sample_weight = None, labels = None)\n",
    "print(\"cross entropy = \" + str(loss)) # the mean loss per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XX_test=X_test.copy()\n",
    "XX_test=pd.DataFrame(XX_test)\n",
    "dtest = xgb.DMatrix(XX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"submission_file.csv\", 'w') as f:\n",
    "    f.write(\"Id,Score\\n\")\n",
    "    for i in range(len(y_pred)):\n",
    "        f.write(str(i)\n",
    "                +','\n",
    "                +str(y_pred[i])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20179"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
